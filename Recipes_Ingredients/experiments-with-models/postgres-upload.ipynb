{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e77092b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\users\\turgu\\anaconda3\\envs\\llms\\lib\\site-packages (2.0.38)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\turgu\\anaconda3\\envs\\llms\\lib\\site-packages (from sqlalchemy) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\turgu\\anaconda3\\envs\\llms\\lib\\site-packages (from sqlalchemy) (4.12.2)\n",
      "Downloading psycopg2_binary-2.9.10-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sqlalchemy psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f05f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('recipes_revisited.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb7760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgresql+psycopg2://postgres:turgutcem@localhost:5432/recipes_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbbe1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Global counters\n",
    "list_conversion_stats = {\"tags\": {\"success\": 0, \"fail\": 0}, \"ingredients\": {\"success\": 0, \"fail\": 0}}\n",
    "json_conversion_stats = {\"amounts\": {\"success\": 0, \"fail\": 0}, \"amount_gram\": {\"success\": 0, \"fail\": 0}}\n",
    "\n",
    "def try_parse_list(x, col):\n",
    "    if isinstance(x, list):\n",
    "        list_conversion_stats[col][\"success\"] += 1\n",
    "        return x\n",
    "    try:\n",
    "        parsed = ast.literal_eval(x)\n",
    "        if isinstance(parsed, list):\n",
    "            list_conversion_stats[col][\"success\"] += 1\n",
    "            return parsed\n",
    "    except:\n",
    "        pass\n",
    "    list_conversion_stats[col][\"fail\"] += 1\n",
    "    return []\n",
    "\n",
    "def try_parse_json(x, col):\n",
    "    if isinstance(x, (dict, list)):\n",
    "        json_conversion_stats[col][\"success\"] += 1\n",
    "        return json.dumps(x)\n",
    "    try:\n",
    "        parsed = ast.literal_eval(x)\n",
    "        if isinstance(parsed, (dict, list)):\n",
    "            json_conversion_stats[col][\"success\"] += 1\n",
    "            return json.dumps(parsed)\n",
    "    except:\n",
    "        pass\n",
    "    json_conversion_stats[col][\"fail\"] += 1\n",
    "    return json.dumps([])\n",
    "\n",
    "def preprocess_for_postgres(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    for col in [\"tags\", \"ingredients\"]:\n",
    "        df[col] = df[col].apply(lambda x: try_parse_list(x, col))\n",
    "\n",
    "    for col in [\"amounts\", \"amount_gram\"]:\n",
    "        df[col] = df[col].apply(lambda x: try_parse_json(x, col))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78c959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List Conversion Stats:\n",
      "tags: Success=113784  Fail=0\n",
      "ingredients: Success=113784  Fail=0\n",
      "\n",
      "JSON Conversion Stats:\n",
      "amounts: Success=113784  Fail=0\n",
      "amount_gram: Success=113784  Fail=0\n"
     ]
    }
   ],
   "source": [
    "df_clean = preprocess_for_postgres(df)\n",
    "\n",
    "# Print conversion summaries\n",
    "print(\"List Conversion Stats:\")\n",
    "for k, v in list_conversion_stats.items():\n",
    "    print(f\"{k}: Success={v['success']}  Fail={v['fail']}\")\n",
    "\n",
    "print(\"\\nJSON Conversion Stats:\")\n",
    "for k, v in json_conversion_stats.items():\n",
    "    print(f\"{k}: Success={v['success']}  Fail={v['fail']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d902de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\n",
    "    \"postgresql+psycopg2://postgres:turgutcem@localhost:5432/recipes_db?client_encoding=utf8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f1f79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_insert = [\n",
    "    \"recipe_id\", \"name\", \"description\", \"ingredients_raw\", \"steps\",\n",
    "    \"servings\", \"serving_size\", \"tags\", \"ingredients\",\n",
    "    \"amounts\", \"amount_gram\", \"serving_size_numeric\",\n",
    "    \"total_recipe_weight\", \"recipe_energy_kcal_per100g\",\n",
    "    \"recipe_energy_kcal_per_serving\"\n",
    "]\n",
    "df_clean = df_clean.rename(columns={\"id\": \"recipe_id\"})\n",
    "\n",
    "df_clean[columns_to_insert].to_sql(\"recipes\", engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "399b4b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in df_clean: 113784\n",
      "Rows in DB: 113784\n"
     ]
    }
   ],
   "source": [
    "print(\"Total rows in df_clean:\", len(df_clean))\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM recipes;\"))\n",
    "    print(\"Rows in DB:\", result.scalar_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58c62aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device name: NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c328b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c673a59e19846a0b9e70e65dd625cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1778 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model = model.to(\"cuda\")  # Push model to GPU\n",
    "\n",
    "embeddings = model.encode(\n",
    "    df_clean[\"name_description\"].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    device=\"cuda\"  # Force GPU use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc558446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"embedding\"] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dfaa2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Connect to Postgres\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"recipes_db\",\n",
    "    user=\"postgres\",\n",
    "    password=\"turgutcem\", \n",
    "    host=\"localhost\",\n",
    "    port=5432\n",
    ")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea9388af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of (embedding, recipe_id) pairs\n",
    "data = list(zip(df_clean[\"embedding\"], df_clean[\"recipe_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9af174c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [2:31:04<00:00, 79.52s/it] \n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    UPDATE recipes\n",
    "    SET embedding = %s\n",
    "    WHERE recipe_id = %s;\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 1000 \n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    batch = data[i:i+batch_size]\n",
    "    execute_batch(cur, query, batch)\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3678fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_update = [\"recipe_energy_per100g\", \"recipe_carbohydrates_per100g\", \"recipe_proteins_per100g\", \"recipe_fat_per100g\",\n",
    "    \"recipe_energy_per_serving\", \"recipe_carbohydrates_per_serving\", \"recipe_proteins_per_serving\", \"recipe_fat_per_serving\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bdafe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_native_final(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    elif isinstance(x, (np.generic, np.ndarray)):\n",
    "        return x.item()\n",
    "    return x\n",
    "\n",
    "for col in columns_to_update + [\"recipe_id\"]:\n",
    "    df_clean[col] = df_clean[col].apply(to_native_final).astype(object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0a93725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe_energy_per100g: 837.8910860256908 (<class 'float'>)\n",
      "recipe_carbohydrates_per100g: 8.031474114441416 (<class 'float'>)\n",
      "recipe_proteins_per100g: 12.647609964966913 (<class 'float'>)\n",
      "recipe_fat_per100g: 13.490991825613078 (<class 'float'>)\n",
      "recipe_energy_per_serving: 1734.4345480731802 (<class 'float'>)\n",
      "recipe_carbohydrates_per_serving: 16.62515141689373 (<class 'float'>)\n",
      "recipe_proteins_per_serving: 26.180552627481504 (<class 'float'>)\n",
      "recipe_fat_per_serving: 27.92635307901907 (<class 'float'>)\n",
      "recipe_id: 76133 (<class 'int'>)\n"
     ]
    }
   ],
   "source": [
    "for col in columns_to_update + [\"recipe_id\"]:\n",
    "    sample = df_clean[col].dropna().iloc[0]\n",
    "    print(f\"{col}: {sample} ({type(sample)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ebe766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe_energy_per100g <class 'numpy.float64'>\n",
      "recipe_carbohydrates_per100g <class 'numpy.float64'>\n",
      "recipe_proteins_per100g <class 'numpy.float64'>\n",
      "recipe_fat_per100g <class 'numpy.float64'>\n",
      "recipe_energy_per_serving <class 'numpy.float64'>\n",
      "recipe_carbohydrates_per_serving <class 'numpy.float64'>\n",
      "recipe_proteins_per_serving <class 'numpy.float64'>\n",
      "recipe_fat_per_serving <class 'numpy.float64'>\n",
      "recipe_id <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "for col in columns_to_update + [\"recipe_id\"]:\n",
    "    print(col, type(df_clean[col].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f84135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    tuple(df_clean.loc[i, columns_to_update].tolist() + [df_clean.loc[i, \"recipe_id\"]])\n",
    "    for i in range(len(df_clean))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d811632",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd9def74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [2:40:28<00:00, 84.46s/it] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# SQL update statement\n",
    "set_clause = \", \".join([f\"{col} = %s\" for col in columns_to_update])\n",
    "sql = f\"\"\"\n",
    "    UPDATE recipes\n",
    "    SET {set_clause}\n",
    "    WHERE recipe_id = %s;\n",
    "\"\"\"\n",
    "\n",
    "# Batch update\n",
    "batch_size = 1000\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    try:\n",
    "        batch = data[i:i+batch_size]\n",
    "        execute_batch(cur, sql, batch)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {i}–{i+batch_size}: {e}\")\n",
    "        conn.rollback()\n",
    "        break  # Stop here to inspect\n",
    "\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
