{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc595e0",
   "metadata": {},
   "source": [
    "# Retrospective: Attempts, Issues, and Lessons Learned\n",
    "\n",
    "## 1  Data preparation  \n",
    "- **Recipe dialogues (`qwen_recipe_dialogues.jsonl`)**\n",
    "  - Generated ~4 300 multi-turn dialogues with JSON-only assistant turns.  \n",
    "  - Validation script confirmed schema conformance and length distribution, but later review exposed “silent” errors (e.g., tags such as `kosher` being inserted without user mention).  \n",
    "- **Tag & ingredient metadata**\n",
    "  - Tags (≈ 467) and their group mappings loaded.  \n",
    "  - Embeddings generated with `all-MiniLM-L6-v2`, saved to Postgres `tags` table (`vector` column).  \n",
    "  - Ingredient canonical / variant tables created and populated; ~4 000 canonicals, ~16 000 variants.\n",
    "\n",
    "## 2  Database layer  \n",
    "- Postgres 15 + pgvector extension.\n",
    "- **Schemas implemented**\n",
    "  - `tags(tag_id PK, tag_name, group_name, embedding vector(384))`.\n",
    "  - `tag_groups(group_name PK, embedding vector(384))`.\n",
    "  - `ingredients(ing_id PK, canonical, embedding vector(384))`.\n",
    "  - `ingredient_variants(variant TEXT PK, canonical REFERENCES ingredients)`.\n",
    "- **Issues encountered**\n",
    "  - Attempted vector slicing in SQL (`embedding[1:3]`) — pgvector does not support subscripting; fixed by using `vector_dims(embedding)` or exporting to Python for preview.  \n",
    "  - Unicode errors during `execute_values`; solved by ensuring UTF-8 connection and avoiding `.tolist()` (stored Python list → JSONB instead).\n",
    "\n",
    "## 3  Model fine-tuning attempts  \n",
    "\n",
    "| Step | Action | Outcome / Error |\n",
    "|------|--------|-----------------|\n",
    "| 3-A | Loaded **Qwen 2.5-0.5B-Instruct** in 4-bit with LoRA config (6.6 M trainable params). | Successful. |\n",
    "| 3-B | Tokenised dialogues (train = 4 203, val = 131). | OK. |\n",
    "| 3-C | `TrainingArguments` error (`evaluation_strategy` unknown). | Local transformers version 4.53.0 lacked that kwarg; upgraded to 4.53.1, then switched to `eval_strategy`. |\n",
    "| 3-D | LoRA fine-tuning completed (2 epochs, loss ↓ from 0.2528 → 0.2291). | Training finished; checkpoints in `qwen2p5-recipe-lora` and `…-final`. |\n",
    "| 3-E | Merged LoRA into base → `qwen2p5-recipe-merged`. | `tokenizer` files missing; copied manually. |\n",
    "| 3-F | Inference test with merged model + base tokenizer. | Output not JSON-compliant; hallucinated duplicate keys, comments, and invalid structure. |\n",
    "| 3-G | Added explicit system JSON-rules prompt + `chat_template`. | Improved but still produced malformed JSON (duplicate keys, trailing commas) for edge prompts. |\n",
    "\n",
    "## 4  Automated post-processing / cleaning  \n",
    "- Wrote `clean_conversation_with_api` to call GPT-4o-mini for:  \n",
    "  1. Removing unseen tags / ingredients.  \n",
    "  2. Rephrasing first-turn user opener (avoid repetition with sliding window of 5 past openers).  \n",
    "- Problems:  \n",
    "  - ChatGPT occasionally introduced new errors (e.g., duplicated keys) despite JSON formatting request.  \n",
    "  - “Semantic” deletion rule (e.g., dropping `kosher` when not implied) remained unreliable.\n",
    "\n",
    "## 5  Failure Modes Identified  \n",
    "\n",
    "1. **Tokenizer / model mismatch after merge**  \n",
    "   - Merged directory lacked complete tokenizer assets; `AutoTokenizer` could not locate `vocab.json` → `NoneType` error.  \n",
    "\n",
    "2. **TrainingArguments API drift**  \n",
    "   - Multiple errors due to version mismatches (`evaluation_strategy`, `eval_strategy`).  \n",
    "\n",
    "3. **Generation quality**  \n",
    "   - Even with LoRA, model often violates JSON constraints:  \n",
    "     - Duplicate keys (`include_ingredients` twice).  \n",
    "     - In-line comments (`// …`).  \n",
    "     - Hallucinated tags or ingredients.  \n",
    "\n",
    "4. **Post-processing loop**  \n",
    "   - Clean-up prompts insufficiently constrained; still permitted undesired additions.  \n",
    "   - Reliance on GPT-API for deterministic cleansing proved fragile.\n",
    "\n",
    "## 6  Lessons & Next Steps  \n",
    "\n",
    "- **Tokenizer integrity**: Always copy full tokenizer set (`*.json`, `*.model`, special token maps) when saving merged checkpoints.  \n",
    "- **Strict decoding**: Use `model.generate(..., decoder_start_token_id, eos_token_id, logits_processor=[...JSONProcessor])` or enforce incremental validation rather than post-hoc regex fixes.  \n",
    "- **Structured fine-tuning**:  \n",
    "  - Consider **direct preference optimization** (DPO / RLHF) on JSON validity instead of LoRA alone.  \n",
    "  - Smaller curated dataset focusing on tricky cases (exclusions, resets).  \n",
    "- **Evaluation harness**: Build automatic JSON validator & semantic checker (tag-source alignment) to score generation before deployment.  \n",
    "- **Version pinning**: Lock `transformers`, `peft`, `bitsandbytes` versions in `requirements.txt` to avoid API drift.\n",
    "- **Model Change**: Consequently, I’ve decided to pivot the project to the Llama 3.1 1 B model, confident it will provide a more balanced blend of accuracy, efficiency, and predictable JSON control for our recipe-assistant goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5374162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "# Load tags\n",
    "with open(\"tags\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_tags = f.read().splitlines()\n",
    "\n",
    "# Load ingredient clusters\n",
    "with open(\"ingredient_clusters.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ingredient_clusters = json.load(f)\n",
    "\n",
    "# Load ingredient mapping\n",
    "with open(\"ingredient_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ingredient_mapping = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f822c93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TIME_DURATION',\n",
       "  ['15-minutes-or-less',\n",
       "   '30-minutes-or-less',\n",
       "   '60-minutes-or-less',\n",
       "   '4-hours-or-less',\n",
       "   '1-day-or-more']),\n",
       " ('COMPLEXITY_EASE',\n",
       "  ['3-steps-or-less',\n",
       "   '5-ingredients-or-less',\n",
       "   'easy',\n",
       "   'beginner-cook',\n",
       "   'for-1-or-2',\n",
       "   'for-large-groups',\n",
       "   'one-dish-meal',\n",
       "   'from-scratch']),\n",
       " ('DIETARY_RESTRICTIONS',\n",
       "  ['vegan',\n",
       "   'vegetarian',\n",
       "   'gluten-free',\n",
       "   'dairy-free',\n",
       "   'egg-free',\n",
       "   'nut-free',\n",
       "   'lactose',\n",
       "   'kosher',\n",
       "   'diabetic',\n",
       "   'no-shell-fish'])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert TAG_GROUPS = {...} into Python dict\n",
    "tag_string = \"\\n\".join(raw_tags).strip().replace(\"TAG_GROUPS = \", \"\")\n",
    "tag_groups = ast.literal_eval(tag_string)\n",
    "\n",
    "list(tag_groups.items())[:3]  # (group_name, [tags])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03670b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15-minutes-or-less',\n",
       " '30-minutes-or-less',\n",
       " '60-minutes-or-less',\n",
       " '4-hours-or-less',\n",
       " '1-day-or-more',\n",
       " '3-steps-or-less',\n",
       " '5-ingredients-or-less',\n",
       " 'easy',\n",
       " 'beginner-cook',\n",
       " 'for-1-or-2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flat list\n",
    "flat_tags = [tag for group in tag_groups.values() for tag in group]\n",
    "\n",
    "flat_tags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb4c9ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('salt',\n",
       "  ['salt', 'saltine', 'nu-salt', \"salt,'\", 'nu salt', 'salt,.', 'salt, &']),\n",
       " ('butter',\n",
       "  ['butter',\n",
       "   'buttermilk',\n",
       "   'cold butter',\n",
       "   'soft butter',\n",
       "   'real butter',\n",
       "   'firm butter',\n",
       "   'nut butter',\n",
       "   'butter buds',\n",
       "   'herb butter',\n",
       "   'shea butter',\n",
       "   'cub butter',\n",
       "   'buttery oil',\n",
       "   'aloe butter',\n",
       "   'ghee butter',\n",
       "   'hard butter',\n",
       "   'hot butter',\n",
       "   'pure butter']),\n",
       " ('eggs', ['eggs', 'egg']),\n",
       " ('sugar', ['sugar', 'raw sugar', 'red sugar', 'sugar*']),\n",
       " ('onion',\n",
       "  ['onion',\n",
       "   'onions',\n",
       "   'red onion',\n",
       "   'dry onion',\n",
       "   'raw onion',\n",
       "   'onion,.',\n",
       "   'onion, --'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert list of dicts to canonical → list of variants\n",
    "ingredient_clusters_dict = {\n",
    "    item[\"canonical\"]: item[\"variants\"]\n",
    "    for item in ingredient_clusters\n",
    "    if \"canonical\" in item and \"variants\" in item\n",
    "}\n",
    "\n",
    "list(ingredient_clusters_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9d79841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188602</td>\n",
       "      <td>[30-minutes-or-less, time-to-make, course, mai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122957</td>\n",
       "      <td>[60-minutes-or-less, time-to-make, course, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106263</td>\n",
       "      <td>[60-minutes-or-less, time-to-make, course, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194121</td>\n",
       "      <td>[15-minutes-or-less, time-to-make, course, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152982</td>\n",
       "      <td>[time-to-make, course, preparation, very-low-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   recipe_id                                               tags\n",
       "0     188602  [30-minutes-or-less, time-to-make, course, mai...\n",
       "1     122957  [60-minutes-or-less, time-to-make, course, pre...\n",
       "2     106263  [60-minutes-or-less, time-to-make, course, pre...\n",
       "3     194121  [15-minutes-or-less, time-to-make, course, pre...\n",
       "4     152982  [time-to-make, course, preparation, very-low-c..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Connect to Postgres\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"recipes_db\",\n",
    "    user=\"postgres\",\n",
    "    password=\"turgutcem\", \n",
    "    host=\"localhost\",\n",
    "    port=5432\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Pull recipe_id and tags (assumed to be stored as text[] or JSONB array)\n",
    "cur.execute(\"SELECT recipe_id, tags FROM recipes;\")\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Build dataframe\n",
    "df_tags_raw = pd.DataFrame(rows, columns=[\"recipe_id\", \"tags\"])\n",
    "df_tags_raw[\"tags\"] = df_tags_raw[\"tags\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "df_tags_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a898cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "recipes_df = pd.read_csv(r'C:\\Users\\turgu\\sertifika\\reciperesuggestion\\Recipes_Ingredients\\experiments-with-models\\recipes_revisited.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7293186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid tags: 467\n",
      "Sample: ['1-day-or-more', '15-minutes-or-less', '3-steps-or-less', '30-minutes-or-less', '4-hours-or-less', '5-ingredients-or-less', '60-minutes-or-less', 'a1-sauce', 'african', 'american']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Load smart tag groups\n",
    "with open(\"smart_tag_groups.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    smart_tag_groups = json.load(f)\n",
    "\n",
    "# Flatten into a list of valid tags\n",
    "valid_tags = sorted({tag for group_tags in smart_tag_groups.values() for tag in group_tags})\n",
    "\n",
    "# Reverse lookup from tag → group\n",
    "tag_to_group = {}\n",
    "for group_name, tags in smart_tag_groups.items():\n",
    "    for tag in tags:\n",
    "        tag_to_group[tag] = group_name\n",
    "\n",
    "print(f\"Valid tags: {len(valid_tags)}\")\n",
    "print(\"Sample:\", valid_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df599c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60-minutes-or-less: [76133, 318331, 164054, 339949, 250990, 420900, 504881, 136221, 106968, 230280]\n",
      "casseroles: [76133, 160997, 413140, 197205, 441130, 329869, 494141, 481407, 54875, 251747]\n",
      "main-dish: [76133, 489452, 8312, 164054, 214352, 250990, 102427, 49313, 136221, 106968]\n",
      "eggs-dairy: [76133, 325623, 205188, 304398, 372781, 23889, 619, 239089, 413140, 78364]\n",
      "oven: [76133, 318331, 60921, 160997, 372781, 23889, 413140, 78364, 134654, 59828]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Tag index: tag -> list of recipe IDs that have it\n",
    "tag_to_recipe_ids = defaultdict(list)\n",
    "\n",
    "for idx, row in recipes_df.iterrows():\n",
    "    recipe_id = row[\"id\"]\n",
    "    try:\n",
    "        recipe_tags = ast.literal_eval(row[\"tags\"]) if isinstance(row[\"tags\"], str) else row[\"tags\"]\n",
    "    except:\n",
    "        continue\n",
    "    for tag in recipe_tags:\n",
    "        if tag in valid_tags:\n",
    "            tag_to_recipe_ids[tag].append(recipe_id)\n",
    "\n",
    "# Filter to max 10 recipes per tag\n",
    "tag_to_recipe_ids_10 = {tag: ids[:10] for tag, ids in tag_to_recipe_ids.items()}\n",
    "\n",
    "# Preview\n",
    "for tag, ids in list(tag_to_recipe_ids_10.items())[:5]:\n",
    "    print(f\"{tag}: {ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "274b5606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\turgu\\anaconda3\\envs\\llms\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d5c6be15744afe9fb847124319a693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag embeddings shape: torch.Size([467, 384])\n",
      "df_tag_embeddings created — sample:\n",
      "                         0         1         2         3         4    \\\n",
      "1-day-or-more      -0.026001  0.048450  0.048305  0.040473 -0.030104   \n",
      "15-minutes-or-less  0.015296  0.079017  0.023351 -0.061849  0.031248   \n",
      "3-steps-or-less    -0.016551  0.045348 -0.015148 -0.059154 -0.001241   \n",
      "30-minutes-or-less  0.044542  0.042507 -0.015222 -0.046406  0.022921   \n",
      "4-hours-or-less     0.079789  0.066414  0.038017  0.015312  0.016564   \n",
      "\n",
      "                         5         6         7         8         9    ...  \\\n",
      "1-day-or-more      -0.061308  0.014986 -0.013676 -0.013961 -0.028035  ...   \n",
      "15-minutes-or-less -0.003695 -0.059204  0.012238  0.096509 -0.032138  ...   \n",
      "3-steps-or-less     0.007241 -0.087239  0.009688 -0.018296  0.016620  ...   \n",
      "30-minutes-or-less  0.016567 -0.054062  0.013243  0.072070  0.013745  ...   \n",
      "4-hours-or-less    -0.039805 -0.078136  0.036589  0.004458  0.007705  ...   \n",
      "\n",
      "                         374       375       376       377       378  \\\n",
      "1-day-or-more       0.023229  0.072010 -0.018294 -0.017429 -0.016512   \n",
      "15-minutes-or-less  0.118813  0.001897  0.026032 -0.072831 -0.005257   \n",
      "3-steps-or-less     0.100936 -0.021586 -0.026741 -0.041530 -0.044886   \n",
      "30-minutes-or-less  0.089982 -0.000547  0.013383 -0.086312 -0.014197   \n",
      "4-hours-or-less     0.078950  0.010219  0.078468 -0.012015  0.006103   \n",
      "\n",
      "                         379       380       381       382       383  \n",
      "1-day-or-more      -0.004847 -0.056618 -0.043741 -0.048064 -0.022298  \n",
      "15-minutes-or-less -0.031199 -0.015681 -0.018967 -0.074782 -0.029738  \n",
      "3-steps-or-less     0.090467  0.056724 -0.005179  0.046095 -0.017424  \n",
      "30-minutes-or-less -0.038415 -0.018661 -0.003235 -0.038623 -0.050136  \n",
      "4-hours-or-less    -0.028675  0.005073 -0.061067 -0.056213 -0.035848  \n",
      "\n",
      "[5 rows x 384 columns]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Generate tag embeddings (these are individual tag names like 'vegetarian', 'soups-stews', etc.)\n",
    "tag_embeddings = model.encode(valid_tags, convert_to_tensor=True, show_progress_bar=True)\n",
    "print(\"Tag embeddings shape:\", tag_embeddings.shape)\n",
    "\n",
    "# Store in DataFrame for later filtering\n",
    "df_tag_embeddings = pd.DataFrame(tag_embeddings.cpu().numpy(), index=valid_tags)\n",
    "print(\"df_tag_embeddings created — sample:\")\n",
    "print(df_tag_embeddings.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c02ccdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group embeddings created by averaging tag vectors\n",
      "Sample: TIME_DURATION        [0.033391405, 0.062393468, 0.016948272, -0.017...\n",
      "DIFFICULTY_SCALE     [-0.027050288, 0.0049353912, -0.011782292, 0.0...\n",
      "DIETARY_HEALTH       [-0.0056575504, 0.011860569, -0.0031483106, 0....\n",
      "CUISINES_REGIONAL    [0.00017722673, 0.036341455, -0.02869772, 0.03...\n",
      "MEAL_COURSES         [-0.026268303, 0.015148821, -0.005991973, 0.03...\n",
      "dtype: object\n",
      "Shape: (384,)\n"
     ]
    }
   ],
   "source": [
    "# Compute group embeddings by averaging member tag vectors\n",
    "group_embedding_dict = {}\n",
    "missing_counts = {}\n",
    "\n",
    "for group_name, tags in smart_tag_groups.items():\n",
    "    vectors = []\n",
    "    missing = []\n",
    "    for tag in tags:\n",
    "        if tag in df_tag_embeddings.index:\n",
    "            vectors.append(df_tag_embeddings.loc[tag].values)\n",
    "        else:\n",
    "            missing.append(tag)\n",
    "\n",
    "    if vectors:\n",
    "        group_embedding_dict[group_name] = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        print(f\"All tags missing for group '{group_name}' — skipping.\")\n",
    "    \n",
    "    if missing:\n",
    "        missing_counts[group_name] = missing\n",
    "\n",
    "# Convert to Series\n",
    "group_embeddings = pd.Series(group_embedding_dict)\n",
    "\n",
    "# Debug: check embedding shape and missing stats\n",
    "print(\"Group embeddings created by averaging tag vectors\")\n",
    "print(\"Sample:\", group_embeddings.head())\n",
    "print(\"Shape:\", group_embeddings.iloc[0].shape if not group_embeddings.empty else None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0baffd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "def find_top_groups(query, group_embeddings, model, top_k=3):\n",
    "    # Encode query\n",
    "    query_emb = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Stack group vectors and compute similarity\n",
    "    group_matrix = torch.tensor(np.stack(group_embeddings.values)).to(query_emb.device)\n",
    "    similarities = util.cos_sim(query_emb, group_matrix)[0]\n",
    "\n",
    "    # Extract top indices\n",
    "    top_indices = torch.topk(similarities, k=top_k).indices.tolist()\n",
    "    top_results = [(group_embeddings.index[i], similarities[i].item()) for i in top_indices]\n",
    "\n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1700db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_tags_across_groups(user_query, group_names, df_tag_embeddings, model, top_k=5):\n",
    "    results = []\n",
    "\n",
    "    # Encode user query once\n",
    "    query_embedding = model.encode(user_query, convert_to_tensor=True)\n",
    "    query_embedding = query_embedding.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    for group_name in group_names:\n",
    "        # Get tags in this group\n",
    "        tags_in_group = smart_tag_groups.get(group_name, [])\n",
    "        df_subset = df_tag_embeddings.loc[df_tag_embeddings.index.intersection(tags_in_group)]\n",
    "\n",
    "        # Sanity check\n",
    "        if df_subset.empty:\n",
    "            print(f\"No valid tag embeddings found for group '{group_name}'\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            tag_vectors = np.vstack(df_subset.values).astype(np.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"Error stacking vectors for group '{group_name}':\", e)\n",
    "            continue\n",
    "\n",
    "        tag_tensor = torch.tensor(tag_vectors).to(query_embedding.device)\n",
    "        similarities = util.cos_sim(query_embedding, tag_tensor)[0]\n",
    "\n",
    "        for i, tag in enumerate(df_subset.index):\n",
    "            results.append((tag, similarities[i].item(), group_name))\n",
    "\n",
    "    # Sort and return top K\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cc73ee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best matching tags across groups:\n",
      "- breakfast (score: 0.628) in MEAL_COURSES\n",
      "- breakfast-eggs (score: 0.607) in MEAL_COURSES\n",
      "- eggs-breakfast (score: 0.599) in MEAL_COURSES\n",
      "- vegan (score: 0.597) in DIETARY_HEALTH\n",
      "- vegetarian (score: 0.551) in DIETARY_HEALTH\n"
     ]
    }
   ],
   "source": [
    "query = \"i want a vegan breakfast\"\n",
    "top_groups = find_top_groups(query, group_embeddings, model, top_k=3)\n",
    "group_names = [g for g, _ in top_groups]\n",
    "\n",
    "best_tags = find_best_tags_across_groups(query, group_names, df_tag_embeddings, model, top_k=5)\n",
    "\n",
    "print(\"Best matching tags across groups:\")\n",
    "for tag, score, group in best_tags:\n",
    "    print(f\"- {tag} (score: {score:.3f}) in {group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e31fe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API connection successful.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "        max_tokens=5\n",
    "    )\n",
    "    print(\"API connection successful.\")\n",
    "except Exception as e:\n",
    "    print(\"API test failed:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "af9e147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a recipe assistant in a multi-turn conversation with a user.\n",
    "\n",
    "Respond **only** in valid JSON after each user turn; no markdown or prose.\n",
    "\n",
    "JSON format:\n",
    "{\n",
    "  \"name_description\": string,\n",
    "  \"include_tags\": [string],        // ≤ 6\n",
    "  \"exclude_tags\": [string],        // ≤ 6\n",
    "  \"include_ingredients\": [string],\n",
    "  \"exclude_ingredients\": [string],\n",
    "  \"count\": integer,                // default 5 (user may request 1-10)\n",
    "  \"reason\": string\n",
    "}\n",
    "\n",
    "Rules\n",
    "- Never hallucinate: add only what the user asks or clearly implies.\n",
    "- Tag / ingredient *mention-before-use*:\n",
    "  • Add a tag only after the user names that tag or an unmistakable synonym\n",
    "    (“quick” ⇒ 60-min tag, “vegan” ⇒ dietary tag, etc.).\n",
    "  • Add an ingredient only after the user names that ingredient.\n",
    "- If the user excludes an ingredient (e.g. “no eggs”):\n",
    "  • Put that word in exclude_ingredients.\n",
    "  • Remove any include_tag that contains that word (e.g. drop “eggs-dairy”) and\n",
    "    never re-add it unless the user later says that tag name.\n",
    "- Allowed tags are only those in TARGET METADATA.\n",
    "- By the final assistant turn:\n",
    "  • include_tags must contain every tag the USER explicitly requested, plus\n",
    "    at most a few helpful extras, but never exceed six.\n",
    "  • Do **not** surface tags the user never mentioned.\n",
    "  • Never remove a user-requested tag unless it now conflicts with an exclusion.\n",
    "- “vegetarian” ⇒ include both “vegetarian” and “vegan”, but do **not** exclude\n",
    "  meat unless the user asks.\n",
    "- The `count` field defaults to 5; change it only when the user explicitly\n",
    "  requests 1–10.\n",
    "- `name_description` must be a concise summary of the user’s current request\n",
    "  (e.g. “Quick Italian beef casserole”), **never** a literal recipe title.\n",
    "- `reason` explains only what changed since last assistant turn.\n",
    "- If the user says “let’s start over” / “reset”, clear all fields and set\n",
    "  count = 5.\n",
    "\n",
    "Output requirements\n",
    "- Strict ONE-LINE JSON (no literal newlines inside string values).\n",
    "- Double-quote all keys and strings.\n",
    "- Lists must be syntactically valid even if empty.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f7e4399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_dialogue_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"produce_dialogue\",\n",
    "        \"description\": \"Return the full multi-turn conversation.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"messages\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"oneOf\": [\n",
    "                            {   # USER TURN\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"role\":    { \"type\":\"string\", \"enum\":[\"user\"] },\n",
    "                                    \"content\": { \"type\":\"string\" }\n",
    "                                },\n",
    "                                \"required\": [\"role\",\"content\"]\n",
    "                            },\n",
    "                            {   # ASSISTANT TURN\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"role\":  { \"type\":\"string\", \"enum\":[\"assistant\"] },    \n",
    "                                    \"name_description\":     { \"type\":\"string\" },\n",
    "                                    \"include_tags\":         { \"type\":\"array\",\"items\":{\"type\":\"string\"} },\n",
    "                                    \"exclude_tags\":         { \"type\":\"array\",\"items\":{\"type\":\"string\"} },\n",
    "                                    \"include_ingredients\":  { \"type\":\"array\",\"items\":{\"type\":\"string\"} },\n",
    "                                    \"exclude_ingredients\":  { \"type\":\"array\",\"items\":{\"type\":\"string\"} },\n",
    "                                    \"count\":                { \"type\":\"integer\" },\n",
    "                                    \"reason\":               { \"type\":\"string\" }\n",
    "                                },\n",
    "                                \"required\": [\n",
    "                                    \"role\",                         \n",
    "                                    \"name_description\",\"include_tags\",\"exclude_tags\",\n",
    "                                    \"include_ingredients\",\"exclude_ingredients\",\"count\",\"reason\"\n",
    "                                ]\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    \"minItems\": 1\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"messages\"]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67528ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "# Canonicalise\n",
    "INGR_CLUSTER_PATH = Path(\"ingredient_clusters.json\")\n",
    "\n",
    "with open(INGR_CLUSTER_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    ingr_clusters = json.load(f) \n",
    "\n",
    "# Reverse map: variant -> canonical\n",
    "variant2canon = {}\n",
    "for entry in ingr_clusters:              \n",
    "    canon = entry[\"canonical\"]\n",
    "    for v in entry[\"variants\"]:\n",
    "        variant2canon[v.lower()] = canon\n",
    "    variant2canon[canon.lower()] = canon\n",
    "\n",
    "def canonicalize_ingredients(raw_ingredients):\n",
    "    \"\"\"Return unique canonical ingredient names, preserving order.\"\"\"\n",
    "    canon_seen = OrderedDict()\n",
    "    for item in raw_ingredients:\n",
    "        canon = variant2canon.get(item.lower(), item.lower())\n",
    "        canon_seen[canon] = None\n",
    "    return list(canon_seen.keys())\n",
    "\n",
    "# Build a mapping tag -> group once, using smart_tag_groups.json you already loaded\n",
    "# Example: tag_group_map = {\"vegan\": \"DIETARY_HEALTH\", ...}\n",
    "\n",
    "def choose_top_k_tags(recipe_tags, tag_group_map, k=6):\n",
    "    recipe_tags = [t for t in recipe_tags if t in tag_group_map]\n",
    "    chosen = []\n",
    "    seen_groups = set()\n",
    "\n",
    "    # one per group\n",
    "    for tag in recipe_tags:\n",
    "        group = tag_group_map.get(tag, None)\n",
    "        if group and group not in seen_groups:\n",
    "            chosen.append(tag)\n",
    "            seen_groups.add(group)\n",
    "            if len(chosen) == k:\n",
    "                return chosen\n",
    "\n",
    "    # fill up to k\n",
    "    for tag in recipe_tags:\n",
    "        if tag not in chosen:\n",
    "            chosen.append(tag)\n",
    "            if len(chosen) == k:\n",
    "                break\n",
    "    return chosen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a57154e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen tags : ['grilling', 'main-dish', 'chicken', 'healthy', '60-minutes-or-less', 'low-fat']\n",
      "Canonical ingredients : ['unsalted butter', 'lemon', 'butter', 'chicken']\n"
     ]
    }
   ],
   "source": [
    "sample_tags = [\n",
    "    \"grilling\", \"main-dish\", \"chicken\", \"healthy\",\n",
    "    \"low-fat\", \"dinner-party\", \"60-minutes-or-less\"\n",
    "]\n",
    "sample_ingredients = [\"Unsalted butter\", \"Lemon\", \"cold Butter\", \"CHICKEN\"]\n",
    "\n",
    "print(\"Chosen tags :\", choose_top_k_tags(sample_tags, tag_to_group, 6))\n",
    "print(\"Canonical ingredients :\", canonicalize_ingredients(sample_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98f5f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def pick_exclusions(base_idx, recipe_rows, tag_to_group, max_exc=2):\n",
    "    \"\"\"\n",
    "    base_idx: index of the 'current' recipe within recipe_rows (length 10)\n",
    "    recipe_rows: list-like of 10 recipe DataFrame rows\n",
    "    Returns: exc_tags, exc_ingr (lists, length 0..max_exc)\n",
    "    \"\"\"\n",
    "    # collect tag frequencies across the 10\n",
    "    tag_freq = Counter()\n",
    "    for r in recipe_rows:\n",
    "        tlist = eval(r[\"tags\"]) if isinstance(r[\"tags\"], str) else r[\"tags\"]\n",
    "        tag_freq.update(tlist)\n",
    "\n",
    "    # current recipe tags / ingredients\n",
    "    base_tags = set(eval(recipe_rows[base_idx][\"tags\"]))\n",
    "    base_ingr = set(canonicalize_ingredients(\n",
    "        eval(recipe_rows[base_idx][\"ingredients\"])\n",
    "    ))\n",
    "\n",
    "    # candidate tag exclusions: rare & not in current\n",
    "    rare_tag_candidates = [\n",
    "        t for t, f in tag_freq.items() if f <= 3 and t not in base_tags\n",
    "    ]\n",
    "    random.shuffle(rare_tag_candidates)\n",
    "    exc_tags = rare_tag_candidates[:max_exc]\n",
    "\n",
    "    # ingredient exclusions\n",
    "    ingr_freq = Counter()\n",
    "    for r in recipe_rows:\n",
    "        ingr_freq.update(\n",
    "            canonicalize_ingredients(eval(r[\"ingredients\"]))\n",
    "        )\n",
    "    rare_ingr_candidates = [\n",
    "        i for i, f in ingr_freq.items() if f <= 3 and i not in base_ingr\n",
    "    ]\n",
    "    random.shuffle(rare_ingr_candidates)\n",
    "    exc_ingr = rare_ingr_candidates[:max_exc]\n",
    "\n",
    "    return exc_tags, exc_ingr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa2fa954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_conflicts(tag: str, exc_ingr: list[str]) -> bool:\n",
    "    \"\"\"Return True if any excluded ingredient word appears in the tag string.\"\"\"\n",
    "    tag_lower = tag.lower()\n",
    "    return any(word.lower() in tag_lower for word in exc_ingr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d501afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target_metadata_msg(recipe_row,\n",
    "                              tag_group_map,\n",
    "                              recipe_rows10=None,\n",
    "                              idx_within10: int = 0,\n",
    "                              max_tags: int = 6):\n",
    "    \"\"\"\n",
    "    Build the TARGET-METADATA system message for a single recipe.\n",
    "    \"\"\"\n",
    "    # Representative include tags (<=6)\n",
    "    recipe_tags = ast.literal_eval(recipe_row[\"tags\"]) \\\n",
    "                   if isinstance(recipe_row[\"tags\"], str) else recipe_row[\"tags\"]\n",
    "    inc_tags = choose_top_k_tags(recipe_tags, tag_group_map, k=max_tags)\n",
    "\n",
    "    # Canonical ingredient list\n",
    "    raw_ingr   = ast.literal_eval(recipe_row[\"ingredients\"]) \\\n",
    "                 if isinstance(recipe_row[\"ingredients\"], str) else recipe_row[\"ingredients\"]\n",
    "    canon_ingr = canonicalize_ingredients(raw_ingr)\n",
    "\n",
    "    # Candidate exclusions (if 10-recipe cohort provided) \n",
    "    if recipe_rows10 is not None:\n",
    "        exc_tags, exc_ingr = pick_exclusions(idx_within10,\n",
    "                                             recipe_rows10,\n",
    "                                             tag_to_group,\n",
    "                                             max_exc=2)\n",
    "    else:\n",
    "        exc_tags, exc_ingr = [], []\n",
    "\n",
    "    # Filter out include tags that conflict with excluded ingredients \n",
    "    inc_tags = [t for t in inc_tags if not tag_conflicts(t, exc_ingr)]\n",
    "\n",
    "    # Build name + description \n",
    "    name_part        = str(recipe_row.get(\"name\", \"\")).strip()\n",
    "    description_part = str(recipe_row.get(\"description\", \"\")).strip()\n",
    "    title = \" – \".join(p for p in [name_part, description_part] if p)\n",
    "\n",
    "    # Compose TARGET METADATA message \n",
    "    content = (\n",
    "        \"TARGET METADATA (use gradually, never output >6 tags):\\n\"\n",
    "        f\"- include_tags        : {inc_tags}\\n\"\n",
    "        f\"- exclude_tags        : {exc_tags}\\n\"\n",
    "        f\"- canonical_ingredients: {canon_ingr}\\n\"\n",
    "        f\"- exclude_ingredients : {exc_ingr}\\n\"\n",
    "        f\"- default_count       : 5\\n\"\n",
    "        f\"- name_description    : \\\"{title}\\\"\\n\"\n",
    "    )\n",
    "    return {\"role\": \"system\", \"content\": content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "57cdbffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET METADATA (use gradually, never output >6 tags):\n",
      "- include_tags        : ['60-minutes-or-less', 'casseroles', 'main-dish', 'oven', 'cheese', 'eggs-dairy']\n",
      "- exclude_tags        : []\n",
      "- canonical_ingredients: ['corned beef', 'thousand island dressing', 'sauerkraut', 'swiss cheese', 'bread', 'butter']\n",
      "- exclude_ingredients : []\n",
      "- default_count       : 5\n",
      "- name_description    : \"Reuben and Swiss Casserole Bake – I think this is even better than a reuben sandwich, I bet you will probably eat the whole casserole by yourself! :)\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = recipes_df.iloc[0]\n",
    "meta_msg = build_target_metadata_msg(row, tag_to_group, max_tags=6)\n",
    "print(meta_msg[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b63b0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot = \"\"\"\n",
    "### EXAMPLE_DIALOGUE  – for reference only\n",
    "Do NOT quote or repeat this.  Observe the structure.\n",
    "\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"user\", \"content\": \"I'm looking for a savory pie recipe with ground beef.\" },\n",
    "    { \"role\": \"assistant\",\n",
    "      \"name_description\": \"Savory pie recipe featuring ground beef\",\n",
    "      \"include_tags\": [\"savory-pies\",\"beef\"],\n",
    "      \"exclude_tags\": [],\n",
    "      \"include_ingredients\": [],\n",
    "      \"exclude_ingredients\": [],\n",
    "      \"count\": 5,\n",
    "      \"reason\": \"Initial request for a savory pie with ground beef.\"\n",
    "    },\n",
    "    …\n",
    "    { \"role\": \"user\", \"content\": \"Let's start over.\" },\n",
    "    { \"role\": \"assistant\",\n",
    "      \"name_description\": \"\",\n",
    "      \"include_tags\": [],\n",
    "      \"exclude_tags\": [],\n",
    "      \"include_ingredients\": [],\n",
    "      \"exclude_ingredients\": [],\n",
    "      \"count\": 5,\n",
    "      \"reason\": \"Resetting as requested.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dc314362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  generate_selfplay_dialogue (function-call version) \n",
    "import json, textwrap , datetime\n",
    "\n",
    "def generate_selfplay_dialogue(meta_msg: dict,\n",
    "                               turns: int = 4,\n",
    "                               need_reset: bool = False) -> dict:\n",
    "    reset_instruction = (\n",
    "        \"\\n- After the third user turn, the user says \\\"Let's start over\\\" \"\n",
    "        \"and begins a completely new intent. Assistant must clear and return empty formatted JSON where count = 5(since it is the default value).\"\n",
    "        if need_reset else \"\"\n",
    "    )\n",
    "\n",
    "    task_block = textwrap.dedent(f\"\"\"\n",
    "    TASK:\n",
    "    - Write a {turns}-turn conversation (user + assistant = {turns*2} messages).\n",
    "    - Follow every RULE from the system prompt.\n",
    "    - Return the entire conversation in **one** call to *produce_dialogue*.\n",
    "    • The single argument must be  \"messages\": [...]\n",
    "    • USER turns must contain only:  {{ \"role\": \"user\", \"content\": \"<utterance>\" }}\n",
    "    • ASSISTANT turns must contain: role=\"assistant\" **plus all seven JSON fields**\n",
    "        (name_description, include_tags, exclude_tags, include_ingredients,\n",
    "        exclude_ingredients, count, reason).\n",
    "    - Do not emit any other tool calls or plain text.\n",
    "    {reset_instruction}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            meta_msg,\n",
    "            {\"role\":\"system\",\"content\":system_prompt},\n",
    "            {\"role\":\"system\",\"content\":one_shot},\n",
    "            {\"role\":\"system\",\"content\":task_block}\n",
    "        ],\n",
    "        tools=[recipe_dialogue_tool],\n",
    "        tool_choice=\"auto\",\n",
    "        temperature=0.4,\n",
    "        max_tokens=1500\n",
    "    )\n",
    "\n",
    "    args = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    dialogue = args[\"messages\"]\n",
    "\n",
    "    \n",
    "    return {\"messages\": dialogue}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "da726187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-play conversation produced with 8 messages\n",
      "Saved to JSONL at 2025-07-01T18:16:46.120735\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# QUICK TEST ON ONE RECIPE\n",
    "convo = generate_selfplay_dialogue(meta_msg, turns=4, need_reset=False)\n",
    "print(\"Self-play conversation produced with\", len(convo[\"messages\"]), \"messages\")\n",
    "\n",
    "# APPEND TO JSONL\n",
    "with open(\"qwen_recipe_dialogues.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(convo, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved to JSONL at\", datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "91857066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(\"qwen_recipe_dialogues.jsonl\"):\n",
    "    os.remove(\"qwen_recipe_dialogues.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "153b21dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡  2276 conversations already saved\n",
      "[ 2276]  generating for tag → welsh\n",
      "[ 2280]  generating for tag → omelets-and-frittatas\n",
      "[ 2290]  generating for tag → simply-potatoes\n",
      "[ 2300]  generating for tag → 1-day-or-more\n",
      "[ 2310]  generating for tag → stir-fry\n",
      "[ 2320]  generating for tag → canning\n",
      "[ 2330]  generating for tag → peppers\n",
      "[ 2340]  generating for tag → water-bath\n",
      "[ 2350]  generating for tag → pacific-northwest\n",
      "[ 2360]  generating for tag → wedding\n",
      "[ 2370]  generating for tag → green-yellow-beans\n",
      "[ 2380]  generating for tag → broccoli\n",
      "[ 2390]  generating for tag → french\n",
      "[ 2400]  generating for tag → ontario\n",
      "[ 2410]  generating for tag → new-zealand\n",
      "[ 2420]  generating for tag → grapes\n",
      "[ 2430]  generating for tag → lime\n",
      "[ 2440]  generating for tag → zucchini\n",
      "[ 2450]  generating for tag → pork-sausage\n",
      "[ 2460]  generating for tag → melons\n",
      "[ 2470]  generating for tag → spaghetti\n",
      "[ 2480]  generating for tag → new-years\n",
      "[ 2490]  generating for tag → superbowl\n",
      "[ 2500]  generating for tag → curries\n",
      "[ 2510]  generating for tag → thai\n",
      "[ 2520]  generating for tag → independence-day\n",
      "[ 2530]  generating for tag → freshwater-fish\n",
      "[ 2540]  generating for tag → bass\n",
      "[ 2550]  generating for tag → chili\n",
      "[ 2560]  generating for tag → salmon\n",
      "[ 2570]  generating for tag → smoker\n",
      "[ 2580]  generating for tag → south-american\n",
      "[ 2590]  generating for tag → broil\n",
      "[ 2600]  generating for tag → beef-ribs\n",
      "[ 2610]  generating for tag → roast\n",
      "[ 2620]  generating for tag → jewish-sephardi\n",
      "[ 2630]  generating for tag → japanese\n",
      "[ 2640]  generating for tag → cupcakes\n",
      "[ 2650]  generating for tag → garnishes\n",
      "[ 2660]  generating for tag → high-fiber\n",
      "[ 2670]  generating for tag → rolled-cookies\n",
      "[ 2680]  generating for tag → plums\n",
      "[ 2690]  generating for tag → lentils\n",
      "[ 2700]  generating for tag → indian\n",
      "[ 2710]  generating for tag → hanukkah\n",
      "[ 2720]  generating for tag → clams\n",
      "[ 2730]  generating for tag → oysters\n",
      "[ 2740]  generating for tag → scallops\n",
      "[ 2750]  generating for tag → lasagna\n",
      "[ 2760]  generating for tag → szechuan\n",
      "[ 2770]  generating for tag → steak\n",
      "[ 2780]  generating for tag → californian\n",
      "[ 2790]  generating for tag → crusts-pastry-dough-2\n",
      "[ 2800]  generating for tag → non-alcoholic\n",
      "[ 2810]  generating for tag → chowders\n",
      "[ 2820]  generating for tag → cake-fillings-and-frostings\n",
      "[ 2830]  generating for tag → shakes\n",
      "[ 2840]  generating for tag → long-grain-rice\n",
      "[ 2850]  generating for tag → bread-machine\n",
      "[ 2860]  generating for tag → cherries\n",
      "[ 2870]  generating for tag → norwegian\n",
      "[ 2880]  generating for tag → cinco-de-mayo\n",
      "[ 2890]  generating for tag → penne\n",
      "[ 2900]  generating for tag → peruvian\n",
      "[ 2910]  generating for tag → salsas\n",
      "[ 2920]  generating for tag → native-american\n",
      "[ 2930]  generating for tag → no-shell-fish\n",
      "[ 2940]  generating for tag → octopus\n",
      "[ 2946]  generating for tag → cambodian\n",
      "[ 2956]  generating for tag → wings\n",
      "[ 2966]  generating for tag → stocks\n",
      "[ 2976]  generating for tag → chutneys\n",
      "[ 2986]  generating for tag → crawfish\n",
      "[ 2996]  generating for tag → ravioli-tortellini\n",
      "[ 3006]  generating for tag → dutch\n",
      "[ 3016]  generating for tag → pizza\n",
      "[ 3026]  generating for tag → oatmeal\n",
      "[ 3036]  generating for tag → lettuces\n",
      "[ 3046]  generating for tag → halloween\n",
      "[ 3056]  generating for tag → gumbo\n",
      "[ 3066]  generating for tag → birthday\n",
      "[ 3076]  generating for tag → baking\n",
      "[ 3086]  generating for tag → swedish\n",
      "[ 3096]  generating for tag → scottish\n",
      "[ 3106]  generating for tag → artichoke\n",
      "[ 3116]  generating for tag → jewish-ashkenazi\n",
      "[ 3126]  generating for tag → malaysian\n",
      "[ 3136]  generating for tag → finnish\n",
      "[ 3146]  generating for tag → steam\n",
      "[ 3156]  generating for tag → jellies\n",
      "[ 3166]  generating for tag → hawaiian\n",
      "[ 3176]  generating for tag → cuban\n",
      "[ 3186]  generating for tag → rosh-hashana\n",
      "[ 3196]  generating for tag → polish\n",
      "[ 3206]  generating for tag → czech\n",
      "[ 3216]  generating for tag → beef-sausage\n",
      "[ 3226]  generating for tag → hungarian\n",
      "[ 3236]  generating for tag → portuguese\n",
      "[ 3246]  generating for tag → irish\n",
      "[ 3256]  generating for tag → st-patricks-day\n",
      "[ 3266]  generating for tag → baja\n",
      "[ 3276]  generating for tag → meatloaf\n",
      "[ 3286]  generating for tag → passover\n",
      "[ 3296]  generating for tag → mashed-potatoes\n",
      "[ 3306]  generating for tag → pressure-cooker\n",
      "[ 3316]  generating for tag → sole-and-flounder\n",
      "[ 3326]  generating for tag → belgian\n",
      "[ 3336]  generating for tag → flat-shapes\n",
      "[ 3346]  generating for tag → pumpkin\n",
      "[ 3356]  generating for tag → brazilian\n",
      "[ 3366]  generating for tag → deer\n",
      "[ 3376]  generating for tag → turkish\n",
      "[ 3386]  generating for tag → icelandic\n",
      "[ 3396]  generating for tag → kwanzaa\n",
      "[ 3406]  generating for tag → macaroni-and-cheese\n",
      "[ 3416]  generating for tag → british-columbian\n",
      "[ 3426]  generating for tag → halibut\n",
      "[ 3436]  generating for tag → mardi-gras-carnival\n",
      "[ 3446]  generating for tag → pakistani\n",
      "[ 3456]  generating for tag → colombian\n",
      "[ 3466]  generating for tag → vietnamese\n",
      "[ 3476]  generating for tag → nut-free\n",
      "[ 3486]  generating for tag → mussels\n",
      "[ 3496]  generating for tag → russian\n",
      "[ 3506]  generating for tag → argentine\n",
      "[ 3516]  generating for tag → austrian\n",
      "[ 3526]  generating for tag → super-bowl\n",
      "[ 3536]  generating for tag → ice-cream\n",
      "[ 3546]  generating for tag → pot-pie\n",
      "[ 3556]  generating for tag → duck\n",
      "[ 3566]  generating for tag → quiche\n",
      "[ 3576]  generating for tag → iraqi\n",
      "[ 3586]  generating for tag → sugar-cookies\n",
      "[ 3596]  generating for tag → bok-choys\n",
      "[ 3606]  generating for tag → memorial-day\n",
      "[ 3616]  generating for tag → labor-day\n",
      "[ 3626]  generating for tag → medium-grain-rice\n",
      "[ 3636]  generating for tag → sudanese\n",
      "[ 3642]  generating for tag → beef-organ-meats\n",
      "[ 3652]  generating for tag → beef-liver\n",
      "[ 3662]  generating for tag → chicken-livers\n",
      "[ 3672]  generating for tag → manicotti\n",
      "[ 3682]  generating for tag → veal\n",
      "[ 3692]  generating for tag → chinese-new-year\n",
      "[ 3698]  generating for tag → nepalese\n",
      "[ 3708]  generating for tag → lobster\n",
      "[ 3718]  generating for tag → eggplant\n",
      "[ 3728]  generating for tag → filipino\n",
      "[ 3738]  generating for tag → tilapia\n",
      "[ 3748]  generating for tag → whole-chicken\n",
      "[ 3758]  generating for tag → iranian-persian\n",
      "[ 3768]  generating for tag → south-african\n",
      "[ 3778]  generating for tag → meatballs\n",
      "[ 3788]  generating for tag → dehydrator\n",
      "[ 3798]  generating for tag → catfish\n",
      "[ 3808]  generating for tag → ramadan\n",
      "[ 3818]  generating for tag → mothers-day\n",
      "[ 3828]  generating for tag → fathers-day\n",
      "[ 3838]  generating for tag → soul\n",
      "[ 3848]  generating for tag → trout\n",
      "[ 3858]  generating for tag → sourdough\n",
      "[ 3868]  generating for tag → whole-turkey\n",
      "[ 3878]  generating for tag → costa-rican\n",
      "[ 3886]  generating for tag → burgers\n",
      "[ 3896]  generating for tag → quebec\n",
      "[ 3906]  generating for tag → dairy-free\n",
      "[ 3916]  generating for tag → oaxacan\n",
      "[ 3926]  generating for tag → chilean\n",
      "[ 3936]  generating for tag → chard\n",
      "[ 3946]  generating for tag → mahi-mahi\n",
      "[ 3956]  generating for tag → ethiopian\n",
      "[ 3966]  generating for tag → veggie-burgers\n",
      "[ 3976]  generating for tag → turkey-burgers\n",
      "[ 3986]  generating for tag → quail\n",
      "[ 3996]  generating for tag → lebanese\n",
      "[ 4006]  generating for tag → lemon-desserts\n",
      "[ 4007]  generating for tag → lemon-cake\n",
      "[ 4008]  generating for tag → fillings-and-frostings-chocolate\n",
      "[ 4018]  generating for tag → micro-melanesia\n",
      "[ 4027]  generating for tag → prepared-potatoes\n",
      "[ 4036]  generating for tag → palestinian\n",
      "[ 4046]  generating for tag → a1-sauce\n",
      "[ 4056]  generating for tag → libyan\n",
      "[ 4066]  generating for tag → laotian\n",
      "[ 4076]  generating for tag → whitefish\n",
      "[ 4086]  generating for tag → unprocessed-freezer\n",
      "[ 4096]  generating for tag → georgian\n",
      "[ 4103]  generating for tag → ecuadorean\n",
      "[ 4113]  generating for tag → egyptian\n",
      "[ 4123]  generating for tag → squid\n",
      "[ 4133]  generating for tag → saudi-arabian\n",
      "[ 4143]  generating for tag → collard-greens\n",
      "[ 4153]  generating for tag → rosh-hashanah\n",
      "[ 4163]  generating for tag → brewing\n",
      "[ 4173]  generating for tag → whole-duck\n",
      "[ 4183]  generating for tag → elk\n",
      "[ 4193]  generating for tag → duck-breasts\n",
      "[ 4203]  generating for tag → pressure-canning\n",
      "[ 4213]  generating for tag → pumpkin-bread\n",
      "[ 4214]  generating for tag → bread-pudding\n",
      "[ 4215]  generating for tag → hunan\n",
      "[ 4221]  generating for tag → bear\n",
      "[ 4222]  generating for tag → angolan\n",
      "[ 4224]  generating for tag → april-fools-day\n",
      "[ 4227]  generating for tag → pheasant\n",
      "[ 4237]  generating for tag → mongolian\n",
      "[ 4241]  generating for tag → perch\n",
      "[ 4249]  generating for tag → crock-pot-main-dish\n",
      "[ 4250]  generating for tag → roast-beef-comfort-food\n",
      "[ 4251]  generating for tag → pot-roast\n",
      "[ 4252]  generating for tag → guatemalan\n",
      "[ 4262]  generating for tag → honduran\n",
      "[ 4266]  generating for tag → venezuelan\n",
      "[ 4276]  generating for tag → orange-roughy\n",
      "[ 4286]  generating for tag → somalian\n",
      "[ 4291]  generating for tag → moose\n",
      "[ 4298]  generating for tag → breakfast-eggs\n",
      "[ 4299]  generating for tag → eggs-breakfast\n",
      "[ 4300]  generating for tag → potato-soup\n",
      "[ 4301]  generating for tag → gelatin-fruit\n",
      "[ 4302]  generating for tag → fillings-and-frostings-fruit\n",
      "[ 4303]  generating for tag → spreads-fruit\n",
      "[ 4304]  generating for tag → namibian\n",
      "[ 4307]  generating for tag → fourth-of-july\n",
      "[ 4309]  generating for tag → congolese\n",
      "[ 4311]  generating for tag → dips-lunch-snacks\n",
      "[ 4317]  generating for tag → pickeral\n",
      "[ 4318]  generating for tag → beef-sauces\n",
      "[ 4319]  generating for tag → spaghetti-sauce\n",
      "[ 4320]  generating for tag → goose\n",
      "[ 4321]  generating for tag → beans-soups\n",
      "[ 4323]  generating for tag → bean-soup\n",
      "[ 4325]  generating for tag → navy-bean-soup\n",
      "[ 4326]  generating for tag → irish-st-patricks-day\n",
      "[ 4327]  generating for tag → pasta-elbow-macaroni\n",
      "[ 4329]  generating for tag → margarita\n",
      "[ 4330]  generating for tag → simply-potatoes2\n",
      "[ 4331]  generating for tag → cookies-and-brownies-nuts\n",
      "[ 4332]  generating for tag → brisket\n",
      "[ 4333]  generating for tag → cabbage\n",
      "Saved 4334 dialogues to qwen_recipe_dialogues.jsonl at 2025-07-02T19:22:03.985986\n"
     ]
    }
   ],
   "source": [
    "# MASTER DIALOGUE-GENERATION LOOP\n",
    "\n",
    "import json, os, datetime\n",
    "\n",
    "out_path = \"qwen_recipe_dialogues.jsonl\"\n",
    "\n",
    "#  work out how many conversations already exist\n",
    "completed = 0                                   \n",
    "if os.path.exists(out_path):                    \n",
    "    with open(out_path, encoding=\"utf-8\") as _f:   \n",
    "        completed = sum(1 for _ in _f)             \n",
    "print(f\"➡  {completed} conversations already saved\")  \n",
    "\n",
    "# open file in append-mode if it exists, otherwise create new\n",
    "mode = \"a\" if completed else \"w\"                \n",
    "with open(out_path, mode, encoding=\"utf-8\") as fout:\n",
    "\n",
    "    skip_convos = completed                     \n",
    "    counter     = completed                     \n",
    "\n",
    "    for anchor_tag, id_list in tag_to_recipe_ids_10.items():\n",
    "\n",
    "        cohort_rows = [\n",
    "            recipes_df.loc[recipes_df['id'] == rid].iloc[0]\n",
    "            for rid in id_list\n",
    "        ]\n",
    "\n",
    "        #  skip whole tag if its 10-recipe block is done \n",
    "        if skip_convos >= len(cohort_rows):     \n",
    "            skip_convos -= len(cohort_rows)     \n",
    "            continue                            \n",
    "\n",
    "        # otherwise resume mid-block \n",
    "        start_at   = skip_convos                \n",
    "        skip_convos = 0                         \n",
    "\n",
    "        print(f\"[{counter:>5}]  generating for tag → {anchor_tag}\")\n",
    "\n",
    "        for idx, row in enumerate(cohort_rows[start_at:], start=start_at):\n",
    "\n",
    "            # standard TARGET msg for multi-recipe tags,\n",
    "            # no exclusions + 3-turn chat for singleton-tags\n",
    "            if len(cohort_rows) > 1:\n",
    "                meta_msg = build_target_metadata_msg(\n",
    "                    row, tag_to_group,\n",
    "                    recipe_rows10 = cohort_rows,\n",
    "                    idx_within10  = idx,\n",
    "                    max_tags      = 6\n",
    "                )\n",
    "                turns = 4\n",
    "            else:\n",
    "                meta_msg = build_target_metadata_msg(\n",
    "                    row, tag_to_group,\n",
    "                    recipe_rows10 = None,       # no exclusions\n",
    "                    max_tags      = 6\n",
    "                )\n",
    "                turns = 3                      # shorter dialogue\n",
    "\n",
    "            # every 10th conversation triggers a reset\n",
    "            need_reset_flag = (counter % 10 == 9)\n",
    "\n",
    "            convo = generate_selfplay_dialogue(\n",
    "                meta_msg,\n",
    "                turns      = turns,\n",
    "                need_reset = need_reset_flag,\n",
    "                # other defaults already set (temperature = 0.4)\n",
    "            )\n",
    "\n",
    "            # write one line to JSONL\n",
    "            fout.write(json.dumps(convo, ensure_ascii=False) + \"\\n\")\n",
    "            counter += 1\n",
    "\n",
    "print(f\"Saved {counter} dialogues to {out_path} at \"\n",
    "      f\"{datetime.datetime.now().isoformat()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "079a63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    dbname=\"recipes_db\",\n",
    "    user=\"postgres\",\n",
    "    password=\"turgutcem\",\n",
    "    host=\"localhost\",\n",
    "    port=5432\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tag_groups (\n",
    "    group_name      TEXT PRIMARY KEY,\n",
    "    member_count    INTEGER   NOT NULL,\n",
    "    embedding       VECTOR(384)\n",
    ");\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2bf15284",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d6f4356a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TIME_DURATION', 'DIFFICULTY_SCALE', 'DIETARY_HEALTH',\n",
       "       'CUISINES_REGIONAL', 'MEAL_COURSES', 'MAIN_INGREDIENTS_PROTEINS',\n",
       "       'MAIN_INGREDIENTS_VEGETABLES', 'MAIN_INGREDIENTS_FRUITS',\n",
       "       'MAIN_INGREDIENTS_GRAINS', 'PREPARATION_METHOD', 'OCCASIONS_SEASONS',\n",
       "       'DISH_TYPES', 'MAIN_INGREDIENTS_MISC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_embeddings.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7fbd977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted / updated 13 tag_groups.\n"
     ]
    }
   ],
   "source": [
    "from psycopg2.extras import execute_batch, register_default_json\n",
    "\n",
    "rows_to_insert = [\n",
    "    (\n",
    "        g,                                 # group_name\n",
    "        len(smart_tag_groups[g]),          # member_count\n",
    "        group_embeddings[g].tolist()       # 384-dim vector → Python list\n",
    "    )\n",
    "    for g in group_embeddings.index\n",
    "]\n",
    "\n",
    "sql = \"\"\"\n",
    "INSERT INTO tag_groups (group_name, member_count, embedding)\n",
    "VALUES (%s, %s, %s)\n",
    "ON CONFLICT (group_name) DO UPDATE\n",
    "SET member_count = EXCLUDED.member_count,\n",
    "    embedding     = EXCLUDED.embedding;\n",
    "\"\"\"\n",
    "\n",
    "execute_batch(cur, sql, rows_to_insert)\n",
    "conn.commit()\n",
    "\n",
    "print(f\"Inserted / updated {len(rows_to_insert)} tag_groups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9a2cb040",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"df_tag_embeddings\" in globals(), \"df_tag_embeddings missing!\"\n",
    "assert \"tag_to_group\"       in globals(), \"tag_to_group missing!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cc15207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted / updated 467 tags.\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tags (\n",
    "    tag_name     TEXT PRIMARY KEY,\n",
    "    group_name   TEXT    NOT NULL REFERENCES tag_groups(group_name),\n",
    "    embedding    VECTOR(384)\n",
    ");\n",
    "\"\"\"\n",
    ")\n",
    "conn.commit()\n",
    "\n",
    "# build rows (tag_name, group_name, embedding)\n",
    "rows = [\n",
    "    (\n",
    "        tag,                         # tag_name\n",
    "        tag_to_group[tag],           # group FK\n",
    "        df_tag_embeddings.loc[tag].tolist()   # 384-dim vector\n",
    "    )\n",
    "    for tag in df_tag_embeddings.index\n",
    "]\n",
    "\n",
    "# batch-insert with UPSERT\n",
    "sql = \"\"\"\n",
    "INSERT INTO tags (tag_name, group_name, embedding)\n",
    "VALUES (%s, %s, %s)\n",
    "ON CONFLICT (tag_name) DO UPDATE\n",
    "SET group_name = EXCLUDED.group_name,\n",
    "    embedding  = EXCLUDED.embedding;\n",
    "\"\"\"\n",
    "execute_batch(cur, sql, rows)\n",
    "conn.commit()\n",
    "\n",
    "print(f\"Inserted / updated {len(rows)} tags.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710472a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag_name</th>\n",
       "      <th>group</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-day-or-more</td>\n",
       "      <td>TIME_DURATION</td>\n",
       "      <td>[-0.02600091,0.04844982,0.04830537,0.04047262,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15-minutes-or-less</td>\n",
       "      <td>TIME_DURATION</td>\n",
       "      <td>[0.015296437,0.07901682,0.023351092,-0.0618492...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3-steps-or-less</td>\n",
       "      <td>DIFFICULTY_SCALE</td>\n",
       "      <td>[-0.016551048,0.045348424,-0.015147763,-0.0591...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30-minutes-or-less</td>\n",
       "      <td>TIME_DURATION</td>\n",
       "      <td>[0.044542197,0.042507093,-0.015222352,-0.04640...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4-hours-or-less</td>\n",
       "      <td>TIME_DURATION</td>\n",
       "      <td>[0.07978941,0.06641381,0.038016554,0.015312334...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5-ingredients-or-less</td>\n",
       "      <td>DIFFICULTY_SCALE</td>\n",
       "      <td>[0.021728717,-0.012638898,0.050658286,0.025623...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60-minutes-or-less</td>\n",
       "      <td>TIME_DURATION</td>\n",
       "      <td>[0.05332988,0.07557979,-0.009709317,-0.0345772...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a1-sauce</td>\n",
       "      <td>MAIN_INGREDIENTS_MISC</td>\n",
       "      <td>[-0.114903145,-0.01187302,-0.026469233,0.05337...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tag_name                  group  \\\n",
       "0          1-day-or-more          TIME_DURATION   \n",
       "1     15-minutes-or-less          TIME_DURATION   \n",
       "2        3-steps-or-less       DIFFICULTY_SCALE   \n",
       "3     30-minutes-or-less          TIME_DURATION   \n",
       "4        4-hours-or-less          TIME_DURATION   \n",
       "5  5-ingredients-or-less       DIFFICULTY_SCALE   \n",
       "6     60-minutes-or-less          TIME_DURATION   \n",
       "7               a1-sauce  MAIN_INGREDIENTS_MISC   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.02600091,0.04844982,0.04830537,0.04047262,...  \n",
       "1  [0.015296437,0.07901682,0.023351092,-0.0618492...  \n",
       "2  [-0.016551048,0.045348424,-0.015147763,-0.0591...  \n",
       "3  [0.044542197,0.042507093,-0.015222352,-0.04640...  \n",
       "4  [0.07978941,0.06641381,0.038016554,0.015312334...  \n",
       "5  [0.021728717,-0.012638898,0.050658286,0.025623...  \n",
       "6  [0.05332988,0.07557979,-0.009709317,-0.0345772...  \n",
       "7  [-0.114903145,-0.01187302,-0.026469233,0.05337...  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.rollback() \n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    SELECT tag_name,\n",
    "           group_name,\n",
    "           embedding::text AS emb_text \n",
    "    FROM tags\n",
    "    ORDER BY tag_name\n",
    "    LIMIT 8;\n",
    "\"\"\")\n",
    "\n",
    "pd.DataFrame(cur.fetchall(),\n",
    "             columns=[\"tag_name\",\"group\",\"embedding\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "16a227a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] client_encoding was 'SQL_ASCII' → switching to UTF8\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SHOW client_encoding;\")\n",
    "enc = cur.fetchone()[0]\n",
    "if enc.upper() != \"UTF8\":\n",
    "    print(f\"[INFO] client_encoding was {enc!r} → switching to UTF8\")\n",
    "    conn.set_client_encoding(\"UTF8\")   # psycopg2 method\n",
    "else:\n",
    "    print(\"client_encoding already UTF8 – good to go\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1594a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3996 canonicals and 16371 variants into Postgres.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# prepare data \n",
    "canonicals   = list(ingredient_clusters_dict.keys())\n",
    "emb_matrix   = model.encode(canonicals, convert_to_numpy=True)   # (N,384)\n",
    "\n",
    "# schema \n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ingredients (\n",
    "    id         SERIAL PRIMARY KEY,\n",
    "    canonical  TEXT UNIQUE NOT NULL,\n",
    "    embedding  vector(384)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS ingredient_variants (\n",
    "    canonical_id INT REFERENCES ingredients(id) ON DELETE CASCADE,\n",
    "    variant      TEXT,\n",
    "    PRIMARY KEY  (canonical_id, variant)\n",
    ");\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# bulk-insert canonicals \n",
    "rows = [(c, emb.tolist()) for c, emb in zip(canonicals, emb_matrix)]\n",
    "execute_values(\n",
    "    cur,\n",
    "    \"INSERT INTO ingredients (canonical, embedding) VALUES %s \"\n",
    "    \"ON CONFLICT (canonical) DO NOTHING\",\n",
    "    rows\n",
    ")\n",
    "conn.commit()\n",
    "\n",
    "# fetch id map for variant load \n",
    "cur.execute(\"SELECT id, canonical FROM ingredients;\")\n",
    "canon2id = {canon: _id for _id, canon in cur.fetchall()}\n",
    "\n",
    "# bulk-insert variants \n",
    "var_rows = [\n",
    "    (canon2id[canon], var)\n",
    "    for canon, vars_ in ingredient_clusters_dict.items()\n",
    "    for var in vars_\n",
    "]\n",
    "\n",
    "execute_values(\n",
    "    cur,\n",
    "    \"INSERT INTO ingredient_variants (canonical_id, variant) VALUES %s \"\n",
    "    \"ON CONFLICT DO NOTHING\",\n",
    "    var_rows\n",
    ")\n",
    "conn.commit()\n",
    "\n",
    "print(f\"Loaded {len(canonicals)} canonicals and {len(var_rows)} variants into Postgres.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fecd060c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fdcdf0bb1b4aadb79f224a4e3b18b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with schema problems : [] none\n",
      "Over-long examples         : [] none\n",
      "Dialogue length histogram  : [(8, 3518), (10, 384), (12, 230), (14, 128), (6, 74)]\n",
      "Total dialogues            : 4,334\n"
     ]
    }
   ],
   "source": [
    "# quick dataset sanity-check\n",
    "import json, pathlib, collections\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "jsonl_path = pathlib.Path(\"qwen_recipe_dialogues.jsonl\")\n",
    "assert jsonl_path.exists(), f\"{jsonl_path} not found\"\n",
    "\n",
    "bad_schema, long_examples = [], []\n",
    "max_len   = 1024          # tokens later; here we just count characters\n",
    "counts    = collections.Counter()\n",
    "\n",
    "# quick scan\n",
    "with jsonl_path.open(encoding=\"utf-8\") as f:\n",
    "    for ln, line in enumerate(tqdm(f, desc=\"Scanning\")):\n",
    "        data = json.loads(line)\n",
    "        msgs = data.get(\"messages\", [])\n",
    "        # schema check on every assistant turn\n",
    "        need_keys = {\"name_description\",\"include_tags\",\"exclude_tags\",\n",
    "                     \"include_ingredients\",\"exclude_ingredients\",\n",
    "                     \"count\",\"reason\"}\n",
    "        for m in msgs:\n",
    "            if m[\"role\"] == \"assistant\" and not need_keys.issubset(m):\n",
    "                bad_schema.append(ln+1); break\n",
    "        # approximate length check (chars now, tokens later)\n",
    "        total_chars = sum(len(m.get(\"content\",\"\")) for m in msgs)\n",
    "        if total_chars > max_len*4:        # rough 4-char ≈ 1 token rule-of-thumb\n",
    "            long_examples.append(ln+1)\n",
    "        counts[len(msgs)] += 1             # message-count histogram\n",
    "\n",
    "print(\"Lines with schema problems :\", bad_schema[:5], \"…\" if bad_schema else \"none\")\n",
    "print(\"Over-long examples         :\", long_examples[:5], \"…\" if long_examples else \"none\")\n",
    "print(\"Dialogue length histogram  :\", counts.most_common())\n",
    "print(f\"Total dialogues            : {sum(counts.values()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94572801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers, bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c25593",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce415ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7e59a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,586,368 || all params: 500,619,136 || trainable%: 1.3156\n",
      "Qwen 2.5-0.5B loaded in 4-bit and LoRA-ready.\n"
     ]
    }
   ],
   "source": [
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "                    \"gate_proj\",\"up_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"Qwen 2.5-0.5B loaded in 4-bit and LoRA-ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44cab0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import json, random, pathlib, datasets, torch\n",
    "\n",
    "jsonl_path = pathlib.Path(\"qwen_recipe_dialogues.jsonl\") \n",
    "assert jsonl_path.exists(), f\"{jsonl_path} not found!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "259cad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogue_to_text(record: dict) -> str:\n",
    "    \"\"\"\n",
    "    Convert {\"messages\":[...]} into a single concatenated prompt\n",
    "    that Qwen2.5's tokenizer understands.\n",
    "    \"\"\"\n",
    "    chat = []\n",
    "    for turn in record[\"messages\"]:\n",
    "        role = turn[\"role\"]\n",
    "        # user/assistant content\n",
    "        content = turn[\"content\"] if role == \"user\" else json.dumps(\n",
    "            {k: turn[k] for k in (\n",
    "                \"name_description\",\"include_tags\",\"exclude_tags\",\n",
    "                \"include_ingredients\",\"exclude_ingredients\",\"count\",\"reason\")\n",
    "            },\n",
    "            ensure_ascii=False,\n",
    "        )\n",
    "        chat.append({\"role\": role, \"content\": content})\n",
    "    # Qwen’s to_chat_template handles BOS/EOS & system tag insertion\n",
    "    return tokenizer.apply_chat_template(\n",
    "        chat, tokenize=False, add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d5e4eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Streaming JSONL: 4334it [00:00, 6762.44it/s]\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "with jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Streaming JSONL\"):\n",
    "        record = json.loads(line)\n",
    "        samples.append({\"text\": dialogue_to_text(record)})\n",
    "\n",
    "random.shuffle(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10625fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 4,203 train — 131 valid examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4203\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 131\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx = int(len(samples)*0.97)\n",
    "train_ds = datasets.Dataset.from_list(samples[:split_idx])\n",
    "valid_ds = datasets.Dataset.from_list(samples[split_idx:])\n",
    "data = datasets.DatasetDict({\"train\": train_ds, \"validation\": valid_ds})\n",
    "\n",
    "print(f\"Loaded: {len(train_ds):,} train — {len(valid_ds):,} valid examples\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a87d2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\turgu\\anaconda3\\envs\\llms\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e9167a6f9c46c79eebda984c68c916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ce7ef6443041b4aaf6faa3c5a04b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,586,368 || all params: 500,619,136 || trainable%: 1.3156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\turgu\\anaconda3\\envs\\llms\\Lib\\site-packages\\peft\\mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "c:\\Users\\turgu\\anaconda3\\envs\\llms\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LoRA / Trainer setup and start fine-tuning\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import torch, math\n",
    "\n",
    "max_length = 2048        # truncate long dialogues\n",
    "batch_size = 4           \n",
    "grad_accum = 8           \n",
    "num_epochs = 2           \n",
    "lr          = 2e-5\n",
    "\n",
    "# Tokenise function\n",
    "def tok_fn(example):\n",
    "    ids = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Trainer expects dict of lists / tensors\n",
    "    return {\n",
    "        \"input_ids\":  ids[\"input_ids\"][0],\n",
    "        \"attention_mask\": ids[\"attention_mask\"][0],\n",
    "    }\n",
    "\n",
    "tokenised = data.map(tok_fn, batched=False, remove_columns=[\"text\"])\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer, mlm=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# LoRA config \n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"w2\", \"o_proj\"],  # Qwen block output proj layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model_lora = get_peft_model(model, lora_cfg)\n",
    "model_lora.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d661695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.1\n",
      "(output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str, NoneType] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[list[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Union[NoneType, str, list[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: Optional[bool] = False, include_num_input_tokens_seen: Optional[bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: Optional[bool] = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: Optional[bool] = False, average_tokens_across_devices: Optional[bool] = False) -> None\n"
     ]
    }
   ],
   "source": [
    "import transformers, inspect\n",
    "print(transformers.__version__)\n",
    "print(inspect.signature(transformers.TrainingArguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f5a41a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='264' max='264' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [264/264 4:37:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.252800</td>\n",
       "      <td>0.249361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>0.229135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=264, training_loss=0.378726859887441, metrics={'train_runtime': 16719.773, 'train_samples_per_second': 0.503, 'train_steps_per_second': 0.016, 'total_flos': 8611284810566400.0, 'train_loss': 0.378726859887441, 'epoch': 2.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer \n",
    "steps_per_epoch = math.ceil(len(tokenised[\"train\"]) / (batch_size*grad_accum))\n",
    "logging_steps   = max(1, steps_per_epoch // 10)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir         = \"./qwen2p5-recipe-lora\",\n",
    "    per_device_train_batch_size  = batch_size,\n",
    "    per_device_eval_batch_size   = batch_size,\n",
    "    gradient_accumulation_steps = grad_accum,\n",
    "    learning_rate      = lr,\n",
    "    num_train_epochs   = num_epochs,\n",
    "    warmup_steps       = int(0.05 * steps_per_epoch * num_epochs),\n",
    "    fp16               = torch.cuda.is_available(),\n",
    "    logging_steps      = logging_steps,\n",
    "    eval_strategy= \"epoch\",\n",
    "    save_strategy      = \"epoch\",\n",
    "    save_total_limit   = 2,\n",
    "    report_to          = \"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model_lora,\n",
    "    args            = train_args,\n",
    "    train_dataset   = tokenised[\"train\"],\n",
    "    eval_dataset    = tokenised[\"validation\"],\n",
    "    data_collator   = data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "416e28c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qwen2p5-recipe-lora-final\\\\tokenizer_config.json',\n",
       " 'qwen2p5-recipe-lora-final\\\\special_tokens_map.json',\n",
       " 'qwen2p5-recipe-lora-final\\\\chat_template.jinja',\n",
       " 'qwen2p5-recipe-lora-final\\\\vocab.json',\n",
       " 'qwen2p5-recipe-lora-final\\\\merges.txt',\n",
       " 'qwen2p5-recipe-lora-final\\\\added_tokens.json',\n",
       " 'qwen2p5-recipe-lora-final\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"qwen2p5-recipe-lora-final\")      # adapter & config\n",
    "tokenizer.save_pretrained(\"qwen2p5-recipe-lora-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "641afc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model & tokenizer saved to: c:\\Users\\turgu\\sertifika\\reciperesuggestion\\Recipes_Ingredients\\qwen-exploration\\qwen2p5-recipe-merged\n"
     ]
    }
   ],
   "source": [
    "# merge LoRA back into base weights and copy tokenizer files\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import os, torch\n",
    "\n",
    "base_id    = \"Qwen/Qwen2.5-0.5B-Instruct\"      # HF base model\n",
    "lora_dir   = \"qwen2p5-recipe-lora-final\"       # LoRA checkpoint\n",
    "merged_dir = \"qwen2p5-recipe-merged\"           # final fp16 folder\n",
    "os.makedirs(merged_dir, exist_ok=True)\n",
    "\n",
    "# load base in 4-bit, attach LoRA, merge → fp16\n",
    "base  = AutoModelForCausalLM.from_pretrained(\n",
    "            base_id, load_in_4bit=True,\n",
    "            device_map=\"auto\", trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(base, lora_dir)\n",
    "model = model.merge_and_unload()               # returns plain fp16 weights\n",
    "model.save_pretrained(merged_dir)              # writes config & *.safetensors\n",
    "\n",
    "# save the tokenizer once (writes every needed file)\n",
    "tok = AutoTokenizer.from_pretrained(base_id, trust_remote_code=True)\n",
    "tok.save_pretrained(merged_dir)\n",
    "\n",
    "print(\"Merged model & tokenizer saved to:\", os.path.abspath(merged_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "350291b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_dir = \"qwen2p5-recipe-merged\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, device_map=\"auto\",\n",
    "            torch_dtype=\"auto\", trust_remote_code=True)\n",
    "tok   = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5c4bfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW MODEL OUTPUT\n",
      "{\n",
      "  \"name_description\": \"Recipe for a delicious vegan dish.\",\n",
      "  \"include_ingredients\": [\"Vegan protein powder\", \"Tomato paste\", \"Canned tomatoes\"],\n",
      "  \"count\": 6,\n",
      "  \"reason\": \"Added vegan protein powder and tomato paste as requested.\"\n",
      "}\n",
      " Parsed as JSON. Keys: ['name_description', 'include_ingredients', 'count', 'reason']\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "example_assistant = {\n",
    "  \"name_description\": \"Example template\",\n",
    "  \"include_tags\": [],\n",
    "  \"exclude_tags\": [],\n",
    "  \"include_ingredients\": [],\n",
    "  \"exclude_ingredients\": [],\n",
    "  \"count\": 5,\n",
    "  \"reason\": \"Template with all keys present.\"\n",
    "}\n",
    "system_prompt = textwrap.dedent(\"\"\"\n",
    "You are a recipe assistant in a multi-turn conversation with a user.\n",
    "\n",
    "Respond **only** in valid JSON after each user turn, gradually filling the structure below. No explanations, no markdown, no surrounding text.\n",
    "\n",
    "JSON format:\n",
    "{\n",
    "  \"name_description\": string,\n",
    "  \"include_tags\": [string],        \n",
    "  \"exclude_tags\": [string],        \n",
    "  \"include_ingredients\": [string],\n",
    "  \"exclude_ingredients\": [string],\n",
    "  \"count\": integer,                // default 5, user may request 1-10\n",
    "  \"reason\": string\n",
    "}\n",
    "\n",
    "Rules\n",
    "- Never hallucinate preferences — include only what the user asks or clearly implies.\n",
    "- Tag/ingredient **mention-before-use**:\n",
    "  • Add a tag only after the user says that exact tag or an unmistakable synonym\n",
    "    (e.g. “quick” ⇒ time tag, “vegan” ⇒ dietary tag).\n",
    "  • Add an ingredient only after the user says that ingredient.\n",
    "- If the user excludes an ingredient (e.g. “no eggs”):\n",
    "  • List that word in exclude_ingredients.\n",
    "  • **Remove any include_tag that contains that word** (e.g. drop “eggs-dairy”) and do not\n",
    "    re-add it unless the user later says that tag name.\n",
    "- A tag may appear in include_tags / exclude_tags only if it is in the allowed\n",
    "  tag vocabulary supplied in TARGET METADATA. Ingredient words themselves are **not** tags\n",
    "  unless they are legitimate tag names in that vocabulary.\n",
    "- By the final assistant turn:\n",
    "  • include_tags must contain every tag the user explicitly requested (if any) and may\n",
    "    add others that help fulfill the request, **but the total may never exceed six**.\n",
    "  • Do **not** surface tags the user never mentioned.\n",
    "  • Never remove a user-requested tag unless it now conflicts with an ingredient/tag exclusion.\n",
    "- 'vegetarian' ⇒ include both 'vegetarian' and 'vegan', but do NOT exclude meat unless asked.\n",
    "- The `count` field defaults to 5; change it only when the user explicitly requests 1-10.\n",
    "- The `reason` must briefly explain only what changed since the previous assistant turn.\n",
    "- If the user says “let’s start over”, “reset”, or similar, clear all fields and reset count = 5.\n",
    "\n",
    "Output requirements\n",
    "- Strict one-line JSON (no literal newlines inside string values).\n",
    "- Double-quote all keys and strings.\n",
    "- Keep lists syntactically valid, even if empty.\n",
    "\"\"\").strip()\n",
    "\n",
    "#  create a dialogue to test\n",
    "dialogue = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"assistant\", \"content\": json.dumps(example_assistant)},\n",
    "    {\"role\": \"user\",   \"content\": \"I'm looking for a vegan recipe.\"}\n",
    "]\n",
    "\n",
    "inputs = tok.apply_chat_template(dialogue,\n",
    "                                 add_generation_prompt=True,\n",
    "                                 return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    gen_ids = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.10,            # deterministic for inspection\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "    )\n",
    "\n",
    "response_text = tok.decode(gen_ids[0][inputs.shape[-1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "print(\"RAW MODEL OUTPUT\")\n",
    "print(response_text)\n",
    "\n",
    "# quick JSON parse check\n",
    "try:\n",
    "    parsed = json.loads(response_text)\n",
    "    print(\" Parsed as JSON. Keys:\", list(parsed))\n",
    "except Exception as e:\n",
    "    print(\" Could not parse JSON:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
