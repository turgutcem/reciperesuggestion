# Recipe Chat System Environment Configuration

# Database Configuration
POSTGRES_DB=recipes_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
DATABASE_URL=postgresql://postgres:postgres@localhost:5433/recipes_db

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# Session Configuration
SESSION_STORAGE=memory
SESSION_SECRET=your-secret-key-here-change-in-production

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# Frontend Configuration
FRONTEND_HOST=0.0.0.0
FRONTEND_PORT=8501
BACKEND_URL=http://localhost:8001

# Development Settings
DEBUG=true
LOG_LEVEL=INFO

# ============================================
# OPTIONAL: Langfuse Observability
# ============================================
# Langfuse is disabled by default. The system works perfectly without it.
# To enable LLM observability:
# 1. Set LANGFUSE_ENABLED=true
# 2. Start system with docker-compose up
# 3. Visit http://localhost:3000 to create organization and get API keys
# 4. Add your API keys below
# 5. Restart backend: docker restart recipe_backend

LANGFUSE_ENABLED=false
LANGFUSE_HOST=http://localhost:3000
LANGFUSE_ENVIRONMENT=development

# Add these ONLY if you want to enable Langfuse (get from Langfuse UI)
# LANGFUSE_PUBLIC_KEY=pk-something
# LANGFUSE_SECRET_KEY=sk-something

# Langfuse internal settings (don't change these)
LANGFUSE_NEXTAUTH_SECRET=recipe-chat-secret-change-in-production
LANGFUSE_SALT=recipe-chat-salt-change-in-production
LANGFUSE_ENCRYPTION_KEY=0000000000000000000000000000000000000000000000000000000000000000
LANGFUSE_TELEMETRY_ENABLED=false
LANGFUSE_LOG_LEVEL=warn