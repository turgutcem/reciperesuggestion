# Research Summary for LLM Recipe Suggestion App

**Motivation:** The following research explorations were conducted to inform the development of a smart recipe suggestion application powered by Large Language Models (LLMs). The goal was to build a system that can suggest recipes intelligently, taking into account conversation context and nutritional goals. We investigated three key areas: maintaining LLM state, optimizing meal plans for nutrition, and using agents (including multimodal capabilities) for nutrition-related queries. *(Note: Not all these features will be in the initial app version, but the findings are documented for future reference.)*

## LLM State Configuration (Maintaining Conversational State)

One challenge in an LLM-driven assistant is how to maintain **conversational state** across turns, since base LLMs are stateless. We explored an approach to handle state by using **structured JSON objects** that get updated as the conversation progresses. In this method, the LLM’s output is constrained to a JSON format representing the current state (such as user preferences, selected ingredients, etc.), which the system then feeds back into the next prompt. This provides a reliable memory mechanism outside the model’s hidden state.

- **Stateless vs Stateful:** In a stateless interaction, each LLM response depends only on the prompt (which may include recent history). We want a stateful feel, where the assistant remembers prior choices or information. By externalizing memory into a JSON state, we simulate a stateful conversation without the model inherently remembering anything between turns.
- **JSON as State:** JSON is human-readable and LLM-friendly. We found that by defining a clear JSON schema for the state, we could prompt the LLM to output updates to this JSON after each user turn. For example, the state might keep track of selected dietary preferences or ingredients already used in suggested recipes.
- **Ensuring Well-Formed Output:** A key difficulty was ensuring the LLM always produces valid JSON. We looked into techniques like OpenAI’s function calling and the "Outlines" method (which converts a JSON Schema to a regex to enforce structure) to improve compliance. Robust prompting and few-shot examples were used to get the model to adhere to the JSON format.
- **Outcome:** This research confirmed that maintaining an external structured state is feasible and can **“marry the flexibility of natural language with the rigor of structured data”**. This approach will help the recipe assistant remember context (like user dislikes or prior suggestions) across interactions in a controlled way. It aligns with emerging practices in LLM applications that require memory, without needing complex multi-turn architectures.

## Nutrition Optimization Research (AI-Based Meal Planning)

Another aspect explored was how to generate or select recipes that meet **nutritional targets** (calories and macronutrients). Initial attempts used classical optimization techniques (MILP solvers) and a **genetic algorithm** to assemble meal plans, but the results were not ideal. For instance, a genetic algorithm experiment yielded unrealistic meal combinations (e.g. pairing two desserts in one day) despite meeting macro targets. These limitations led us to investigate modern AI approaches for meal plan optimization.

**Literature Review:** We summarized two recent research papers that treat meal planning as a learning and generation problem rather than strict optimization:

- **Andrikopoulou et al. (2024)** – *“A deep learning approach for automatic meal plan generation”*. This work frames meal planning as a **sequential generation problem**. A hybrid model is used: an **RNN** (Recurrent Neural Network) generates sequences of meals (a daily plan), and a **VAE** (Variational Autoencoder) encodes each meal’s nutritional profile into a latent space. The RNN operates in this learned latent nutrition space to assemble meal plans that are diverse yet nutritionally consistent. Training is done on real meal plan datasets, and the model optimizes a combination of reconstruction loss (to match nutritional targets) and KL-divergence (to keep plans realistic). The result is the ability to generate meal plans that satisfy calorie/macronutrient goals while maintaining variety and realism.
- **Papastratis et al. (2024)** – *“Towards personalized and explainable AI-driven meal planning”*. This work introduces an **interactive, personalized approach**. It uses a **VAE** to embed meals into a latent space similar to the above, but adds a **language model (e.g. ChatGPT)** in the loop. The language model interprets user preferences and dietary requirements provided in natural language and translates them into structured constraints or objectives. Essentially, the LLM acts as a high-level planner that can understand instructions like “high protein, no dairy, 3 meals a day, with lots of variety” and then guides the generative model. The VAE then samples meals that meet the criteria in latent space, and the system optimizes for both adherence to nutritional goals and user preferences (even abstract ones like “preference for diversity”). This approach is also aimed at providing explanations for the choices, making the AI’s suggestions more transparent to the user.

**Key Insights:** Both papers suggest a **two-part architecture** for AI meal planning:
1. A **Generative Model** (e.g. VAE, possibly combined with an RNN) that learns the distribution of valid meals and meal combinations. This model captures nutritional profiles of recipes and how they can combine into balanced days.
2. A **Controller or Planner** (could be a sequence model like an RNN or a prompt-driven LLM) that selects or assembles meals from the generative model’s outputs based on user-specific goals and constraints.

This approach lets the system generate realistic, nutritionally coherent meal plans at scale, rather than brute-forcing through millions of recipe combinations. The research indicates that deep generative models can serve as surrogates for traditional optimization, making the search for optimal meal plans more tractable. It also shows how **user preferences** can be integrated via a language interface or latent conditioning, rather than requiring hard-coding every constraint.

- **Application to Our Project:** Based on these insights, our plan is to steer away from purely algorithmic optimization (which struggled in earlier experiments) and move toward a machine learning-based strategy for recipe suggestions. In practice, this could mean training a model on nutritional data of recipes or using an existing language model to help filter or propose recipes that meet targets. While a full RNN+VAE solution is complex, the idea of leveraging an LLM to interpret goals and then generate or retrieve suitable meals is promising. We likely won’t implement a custom deep model immediately, but these findings inform how we might use embeddings or learned representations of recipes to meet nutritional goals more intelligently than a genetic algorithm could.

## Multimodal Nutrition Agent (Vision + LLM Tool Use)

The third research track looked at building a **nutrition assistant agent** that can handle both text and images (multimodal). The motivation was to answer queries like *“Which of these snacks has more protein?”* where the information might come from **nutrition label images** as well as text. This led us to experiment with an agent-based approach, inspired by a deepset Haystack blog example on a "Multimodal Nutrition Agent".

**Concept:** The agent would be able to:
1. **Store** nutrition label images (e.g., photos of food packages with nutritional information) along with brief text captions or metadata.
2. **Retrieve** the relevant image(s) when a user asks a question about nutrition, using embedding-based similarity search on the captions or OCR-extracted text.
3. **Reason** over the combined information (the retrieved nutrition data *plus* the user’s question) using a vision-capable LLM, and provide an answer in natural language.

We prototyped this using the Haystack framework and a ReAct-style LLM agent:
- We set up a vector store (initially an in-memory store, with plans to use **Postgres + pgvector** for scaling) to index the nutrition labels. Each label image was represented as a document with an embedding (using a sentence transformer for text; images could be handled by storing a Base64 string or a reference).
- For the LLM agent, we used the **fastRAG** agent toolkit with a ReAct prompting strategy (Reason+Act loop). The agent has a custom tool (`DocWithImageHaystackQueryTool`) that, when given a nutrition question, will perform a retrieval query to get the most relevant nutrition label document (which includes the image and text). The result is then fed into a multimodal prompt.
- **Multimodal Prompting:** We planned to use a vision-enabled LLM (such as **Phi-3.5 Vision** or similar open-source Vision-LLaMA models) to process the image content. Haystack’s `MultiModalPromptBuilder` allows injecting the Base64-encoded image and its text into the prompt for the LLM. This means the agent’s final prompt to the LLM includes something like: *Image:* `<nutrition_label_image>`, *Question:* "Which snack has more protein?" and the LLM must reason about it.
- **Current Status:** So far, the vision component has not been fully integrated due to resource constraints. Our tests have been **text-only**, using the label captions and a smaller LLM (TinyLlama or Phi-2) just to validate the retrieval and reasoning logic. The agent successfully retrieves the correct text snippet for questions (e.g., comparing two products’ protein content) and uses the ReAct format to explain its steps. Once a GPU is available, the plan is to switch to a stronger multimodal model (like Phi-3.5 Vision) to directly analyze images.
- **Insights:** This multi-modal agent approach, while currently on hold, showed a path to answering detailed nutrition questions that pure text systems might miss. It leverages tool use and the strength of specialized models: a vector database for **search**, and an LLM for **reasoning**. This could be a powerful addition to the recipe app in the future (for example, a user could snap a photo of a nutrition label and ask the app to compare it with another recipe or recommend recipes that complement that food).

## Conclusion

In summary, our research spanned (a) **LLM state management**, (b) **AI-driven nutrition optimization**, and (c) **agent-based multimodal reasoning**. These explorations were driven by the overarching goal of creating a smart recipe recommendation system that is context-aware and nutrition-savvy.

While not all of these components will be immediately integrated into the application (for instance, the multi-modal nutrition agent and complex macro optimizations are beyond the first scope), each research provides valuable insights:

- **State management** with JSON will likely be used to make the LLM assistant remember user context in a structured way ([llm-state-configuration.ipynb](https://github.com/turgutcem/reciperesuggestion/blob/master/Recipes_Ingredients/Research/llm-state-configuration.ipynb)).
- **Nutrition optimization** learnings will guide how we incorporate health goals in recipe suggestions, possibly using embeddings or LLM reasoning rather than brute-force methods ([nutrition-optimization-research.ipynb](https://github.com/turgutcem/reciperesuggestion/blob/master/Recipes_Ingredients/Research/nutrition-optimization-research.ipynb), [genetic-algorithm-experiment.ipynb](https://github.com/turgutcem/reciperesuggestion/blob/master/Recipes_Ingredients/Research/genetic-algorithm-experiment.ipynb)).
- **Agent/Multi-modal capabilities** are a forward-looking feature that could enable the app to answer broader food and nutrition questions ([nutr-agent.ipynb](https://github.com/turgutcem/reciperesuggestion/blob/master/Recipes_Ingredients/Research/nutr-agent.ipynb)).

All research details and code experiments are documented in the linked notebooks above. This foundational work will inform iterative improvements as we build out the intelligent recipe suggestion app.