{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Llama 3‑1B – RAG Prototype with Strict JSON Output\n",
    "# ===============================================================\n",
    "# **This notebook now targets Meta’s official Llama 3‑1B checkpoint.**\n",
    "# Nothing else changed: we still enforce JSON via grammar and hit\n",
    "# Postgres with pgvector.  Follow every section top‑to‑bottom.\n",
    "#\n",
    "# --------------------------------------------------------------\n",
    "# 0. One‑time prerequisites (outside Python)\n",
    "# --------------------------------------------------------------\n",
    "#  a. Install / rebuild libraries (run in PowerShell *inside* the\n",
    "#     `llms` Conda env).  CUDA is optional – delete the three `set`\n",
    "#     lines if you only need CPU.\n",
    "#\n",
    "#     set \"CMAKE_ARGS=-DGGML_CUDA=on\"\n",
    "#     set FORCE_CMAKE=1\n",
    "#     python -m pip install --upgrade --no-cache-dir --force-reinstall \\\n",
    "#         llama-cpp-python sentence-transformers psycopg2-binary \\\n",
    "#         huggingface-hub\n",
    "#\n",
    "#  b. **Hugging Face login + license acceptance**\n",
    "#\n",
    "#     1. `huggingface-cli login`  (paste your HF token)\n",
    "#     2. Go to <https://huggingface.co/meta-llama/Meta-Llama-3-1B> and\n",
    "#        click **“Agree and access repository”** (license gating step).\n",
    "#\n",
    "# --------------------------------------------------------------\n",
    "# 1. Download the safetensors weights & convert → GGUF\n",
    "# --------------------------------------------------------------\n",
    "# The following Python cell will:\n",
    "#   • Grab the 2‑part safetensors checkpoint & tokenizer JSONs\n",
    "#   • Use `convert.py` from llama.cpp to make a Q4_0 GGUF file\n",
    "#     (≈600 MB) in `C:\\Users\\turgu\\models\\llama3-1b`.\n",
    "#   • Skip the conversion on subsequent runs if the GGUF already\n",
    "#     exists.\n",
    "#\n",
    "# Requires: llama.cpp repo cloned somewhere.  If you’ve never cloned\n",
    "# it, the cell tries to grab it automatically under `%USERPROFILE%\\src`.\n",
    "\n",
    "import os, subprocess, sys, textwrap, shutil, json\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# ----------------------------- Paths -----------------------------\n",
    "HF_REPO = \"meta-llama/Meta-Llama-3-1B\"\n",
    "MODEL_DIR = Path(r\"C:/Users/turgu/models/llama3-1b\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GGUF_PATH = MODEL_DIR / \"llama3-1b.Q4_0.gguf\"\n",
    "\n",
    "# 1‑A. Download safetensors & tokenizer if GGUF missing\n",
    "if not GGUF_PATH.is_file():\n",
    "    print(\"GGUF not found – downloading base weights from Hugging Face…\")\n",
    "    for filename in [\n",
    "        \"model-00001-of-00002.safetensors\",\n",
    "        \"model-00002-of-00002.safetensors\",\n",
    "        \"model.safetensors.index.json\",\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer.model\",\n",
    "        \"config.json\",\n",
    "    ]:\n",
    "        local_path = hf_hub_download(repo_id=HF_REPO, filename=filename, local_dir=MODEL_DIR)\n",
    "        print(\"‣\", local_path)\n",
    "\n",
    "    # 1‑B. Ensure llama.cpp repo present (for convert.py)\n",
    "    SRC_DIR = Path(os.environ.get(\"USERPROFILE\", \"~\")) / \"src\"\n",
    "    LLAMA_CPP_DIR = SRC_DIR / \"llama.cpp\"\n",
    "    if not LLAMA_CPP_DIR.exists():\n",
    "        print(\"Cloning llama.cpp repo for conversion script…\")\n",
    "        subprocess.check_call([\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp\", str(LLAMA_CPP_DIR)])\n",
    "\n",
    "    # 1‑C. Run convert.py → GGUF (Q4_0)\n",
    "    print(\"Converting to GGUF (Q4_0)… this may take a few minutes.\")\n",
    "    convert_cmd = [\n",
    "        sys.executable,\n",
    "        str(LLAMA_CPP_DIR / \"convert.py\"),\n",
    "        \"--outfile\", str(GGUF_PATH),\n",
    "        \"--outtype\", \"q4_0\",\n",
    "        str(MODEL_DIR),  # directory containing *.safetensors & config.json\n",
    "    ]\n",
    "    subprocess.check_call(convert_cmd)\n",
    "else:\n",
    "    print(\"✅ GGUF already present →\", GGUF_PATH)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. Prepare tag list (smart_tag_groups.json)\n",
    "# --------------------------------------------------------------\n",
    "TAG_FILE = Path(\"smart_tag_groups.json\")\n",
    "if not TAG_FILE.is_file():\n",
    "    raise FileNotFoundError(\"Place smart_tag_groups.json in the notebook folder or update TAG_FILE path.\")\n",
    "\n",
    "with TAG_FILE.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    tag_data = json.load(f)\n",
    "all_tags = sorted({t for group in tag_data.values() for t in group})\n",
    "print(\"Loaded\", len(all_tags), \"tags.\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. Spin‑up Llama 3‑1B (GGUF)\n",
    "# --------------------------------------------------------------\n",
    "from llama_cpp import Llama, LlamaGrammar\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=str(GGUF_PATH),\n",
    "    n_ctx=2048,\n",
    "    n_threads=os.cpu_count() or 8,\n",
    ")\n",
    "print(\"Model loaded ✔️\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4. Build system prompt & GBNF grammar\n",
    "# --------------------------------------------------------------\n",
    "import textwrap, re\n",
    "\n",
    "SYSTEM_PROMPT = textwrap.dedent(f\"\"\"\n",
    "You are a helpful assistant that converts user requests into a structured JSON query for a recipe search system.\n",
    "\n",
    "Respond **only** with a JSON object having **exactly** these keys in this order:\n",
    "- name_description (string)\n",
    "- include_tags (list of strings)\n",
    "- exclude_tags (list of strings)\n",
    "- include_ingredients (list of strings)\n",
    "- exclude_ingredients (list of strings)\n",
    "- count (integer, default 5 if user gave no number)\n",
    "- reason (string, ≤ 25 words)\n",
    "\n",
    "Only use tags from the following approved list (467 total):\n",
    "{', '.join(all_tags[:50])}, … and {len(all_tags) - 50} more.\n",
    "\"\"\")\n",
    "\n",
    "# Auto‑generate tag alternatives for the grammar\n",
    "TAG_ALTS = \" | \".join(f'\"{t}\"' for t in all_tags)\n",
    "\n",
    "GBNF = textwrap.dedent(f\"\"\"\n",
    "root        ::= object\n",
    "object      ::= \"{\" kvs \"}\"\n",
    "kvs         ::= kv_name \",\" kv_inc_t \",\" kv_exc_t \",\" kv_inc_i \",\" kv_exc_i \",\" kv_count \",\" kv_reason\n",
    "kv_name     ::= \"\\\"name_description\\\" :\" ws? string\n",
    "kv_inc_t    ::= \"\\\"include_tags\\\" :\" ws? list_tag\n",
    "kv_exc_t    ::= \"\\\"exclude_tags\\\" :\" ws? list_tag\n",
    "kv_inc_i    ::= \"\\\"include_ingredients\\\" :\" ws? list_str\n",
    "kv_exc_i    ::= \"\\\"exclude_ingredients\\\" :\" ws? list_str\n",
    "kv_count    ::= \"\\\"count\\\" :\" ws? int\n",
    "kv_reason   ::= \"\\\"reason\\\" :\" ws? string\n",
    "list_tag    ::= \"[\" ws? ( tag ( ws? \",\" ws? tag )* )? ws? \"]\"\n",
    "tag         ::= {TAG_ALTS}\n",
    "list_str    ::= \"[\" ws? ( string ( ws? \",\" ws? string )* )? ws? \"]\"\n",
    "string      ::= \"\\\"\" chars* \"\\\"\"\n",
    "chars       ::= [^\"\\\\] | escape\n",
    "escape      ::= \"\\\\\\\\\".\n",
    "int         ::= digit+\n",
    "digit       ::= [0-9]\n",
    "ws          ::= [ \\t\\n\\r]+\n",
    "\"\"\")\n",
    "\n",
    "grammar = LlamaGrammar.from_string(GBNF)\n",
    "print(\"Grammar compiled ✔️ (length:\", len(GBNF.splitlines()), \"lines)\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 5. Example generation\n",
    "# --------------------------------------------------------------\n",
    "user_input = \"I'm looking for a savory pie recipe with ground beef.\"\n",
    "prompt = f\"\"\"### SYSTEM\\n{SYSTEM_PROMPT}\\n\\n### USER\\n{user_input}\\n\\n### ASSISTANT\\n\"\"\"\n",
    "\n",
    "response = llm(prompt, grammar=grammar, max_tokens=256, temperature=0.3)\n",
    "assistant_json = response[\"choices\"][0][\"text\"].strip()\n",
    "print(\"Raw assistant output →\", assistant_json[:120], \"…\")\n",
    "\n",
    "import json, pprint\n",
    "parsed = json.loads(assistant_json)\n",
    "pprint.pprint(parsed, width=120)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 6. Postgres + pgvector retrieval helper (fill in creds)\n",
    "# --------------------------------------------------------------\n",
    "import psycopg2, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDER = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "PG_CONN_STR = \"dbname=recipes user=postgres password=yourpw\"  # ← adjust\n",
    "\n",
    "\n",
    "def search_recipes(query: dict):\n",
    "    conn = psycopg2.connect(PG_CONN_STR)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    inc_tags, exc_tags = query[\"include_tags\"], query[\"exclude_tags\"]\n",
    "    inc_ing, exc_ing = query[\"include_ingredients\"], query[\"exclude_ingredients\"]\n",
    "\n",
    "    tag_filter = \"\"\n",
    "    if inc_tags:\n",
    "        tag_filter += \" AND tags @> %s\"\n",
    "    if exc_tags:\n",
    "        tag_filter += \" AND NOT tags && %s\"\n",
    "\n",
    "    ing_filter = \"\"\n",
    "    if inc_ing:\n",
    "        ing_filter += \" AND ingredients @> %s\"\n",
    "    if exc_ing:\n",
    "        ing_filter += \" AND NOT ingredients && %s\"\n",
    "\n",
    "    q_vec = EMBEDDER.encode(query[\"name_description\"]).astype(\"float32\")\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        SELECT name, description\n",
    "        FROM recipe\n",
    "        WHERE TRUE {tag_filter} {ing_filter}\n",
    "        ORDER BY embedding <-> %s\n",
    "        LIMIT %s;\n",
    "    \"\"\"\n",
    "\n",
    "    params = []\n",
    "    if inc_tags: params.append(inc_tags)\n",
    "    if exc_tags: params.append(exc_tags)\n",
    "    if inc_ing:  params.append(inc_ing)\n",
    "    if exc_ing:  params.append(exc_ing)\n",
    "    params += [q_vec, query[\"count\"]]\n",
    "\n",
    "    cur.execute(sql, params)\n",
    "    rows = cur.fetchall()\n",
    "    cur.close(); conn.close()\n",
    "    return rows\n",
    "\n",
    "print(\"\\nTop recipes →\")\n",
    "for r in search_recipes(parsed):\n",
    "    print(\"-\", r[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
